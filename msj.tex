\documentclass[sigconf]{acmart}
% \documentclass{...}
%\input{./def/yf-formatting}
\input{./def/yf-def}

%=================================
%Yufei's stuff
%\usepackage{amsmath}
\usepackage{balance}
%\usepackage{times}
\usepackage{microtype}

\def\vgap{\vspace{0mm}}
\def\extraspacing{\vspace{1mm} \noindent}
\def\figcapup{\vspace{-2mm}}
\def\figcapdown{\vspace{-4mm}}

\def\A{\mathcal{A}}
\def\B{\mathcal{B}}
\def\C{\mathcal{C}}
\def\E{\mathcal{E}}
\def\G{\mathcal{G}}
\def\I{\mathcal{I}}
\def\II{\mathscr{I}}
\def\L{\mathcal{L}}
\def\Q{\mathcal{Q}}
\def\R{\mathcal{R}}
\def\T{\mathcal{T}}
\def\U{\mathcal{U}}
\def\V{\mathcal{V}}
\def\X{\mathcal{X}}
\def\XX{\mathscr{X}}
\def\Y{\mathcal{Y}}
\def\YY{\mathscr{Y}}
\def\Z{\mathcal{Z}}

\def\agm{\mathrm{AGM}}
\def\att{\mathbf{att}}
\def\actdom{\mathbf{actdom}}
\def\dom{\mathbf{dom}}
\def\gat{\mit{gat}}
%\def\id{\mathit{id}}
%\def\In{\mathrm{IN}}
\def\heavy{\mit{heavy}}
\def\join{\mathit{Join}}
\def\key{\mathit{key}}
%\def\light{\mathit{light}}
\def\light{\ell}
\def\load{\mathit{load}}
%\def\no{\mathit{no}}
\def\occ{\mathrm{OCC}}
\def\Out{\mathrm{OUT}}
\def\rhostar{{\rho^*}}
\def\schema{\mathit{schema}}
\def\var{\mathit{var}}

\newcommand*{\ldb}{\{\mskip-5mu\{}
\newcommand*{\rdb}{\}\mskip-5mu\}}

\allowdisplaybreaks
%=================================

%====== from acm ======
\acmDOI{}
\acmISBN{}

\acmConference[]{...}{...}{...}
\acmYear{...}
\copyrightyear{}
\acmArticle{}
\acmPrice{}
%======================



\begin{document}
%\begin{sloppy}
    
\title{Parallel Communication Obliviousness: One Round and Beyond}

% \author{}
% \affiliation{
% 	\institution{Chinese University of Hong Kong}
% 	\city{Hong Kong}
% 	\country{China}}	

\author{}


\begin{abstract}
    This paper studies {\em communication-oblivious} algorithms under the massively parallel computation (MPC) model whose communication patterns follow a distribution that depends only on the number $p$ of machines and the input size $N$ of the underlying problem, regardless of the specific input elements. Our objective is to understand when obliviousness necessitates --- or does not necessitate --- heavier communication compared to the traditional MPC model that does not enforce such a requirement.

    \vgap

    The first part of our investigation focuses on single-round algorithms. We prove that {\em skew-free hashing}, a fundamental problem solvable with load $\tO(N/p)$ with high probability (w.h.p.) under the traditional model, demands a load of nearly $\Omega(N)$ under communication obliviousness. Intriguingly, we show that hashing can still be applied in an oblivious manner to process any natural join in one round w.h.p.\ with a load complexity matching that of the best traditional MPC algorithm. The second part of our investigation studies {\em compilation methods} that convert a traditional MPC algorithm $\A$ into a communication-oblivious counterpart. Given an $\A$ that operates within $\ell = \poly(p)$ rounds with a load at most $L = \Omega(p \log p)$, we can produce a communication-oblivious version running in $2\ell$ rounds with a load at most $(1 + \delta) L$, where $\delta > 0$ can be an arbitrarily small constant. Additionally, we establish hardness results indicating that the theoretical guarantees of our compilation can no longer be significantly improved.
\end{abstract}

\maketitle 

\section{Introduction} \label{sec:intro}

In the digital era, cloud computing has emerged as a pivotal part of modern business infrastructure, reshaping the way organizations store, access, and manage data. A growing number of sectors --- ranging from healthcare and finance to academia and entertainment --- are harnessing the power of cloud services to enhance operational efficiency, effectuate cost savings, and fuel innovation. However, the migration of data from local systems to remote servers operated by third-party service providers has ushered in new privacy concerns. The crux of these concerns is the fact that sensitive data, once stored on these servers, is no longer fully under the user's control. Instead, it falls within the stewardship of service providers, thereby raising critical questions about data privacy.

\vgap

Two primary topics come to the forefront in the discourse on privacy control in cloud computing:
\myitems{
	\item {{\bf Confidential computing:}} This aims to mitigate two forms of privacy leakage that might occur on an {\em individual} server. First, rogue administrators could exploit their positions to inspect sensitive data within the server. Second, even if encryption hinders direct data accesses, administrators could still infer valuable information from the memory access patterns that the computation generates.

	\item {{\bf Oblivious communication:}} This aims to prevent another two forms of privacy leakage that could transpire during communication across {\em different} servers. First, malicious network managers might scrutinize the data flowing through the gateways. Second, even with encrypted network traffic, these managers could still deduce sensitive information from the statistics extracted from the communication.
}

Robust solutions to confidential computing are already in place, including systems that employ {\em fully homomorphic encryption} (e.g., IBM's HE4Cloud) or hardware-based {\em trusted execution environments} (nowadays offered by the leading cloud-computing platforms). When integrated with {\em oblivious RAM} techniques \cite{akl+23,go96,ln18,ppry18}, these systems safeguard sensitive information during the entire lifecycle of data: whether the data is stored in the disk, transiting through the memory hierarchy, or in use by the CPU.

\vgap

In this work, we will concentrate on oblivious communication, with the aim to understand how much communication is needed to enforce this requirement. To pave the foundation of our discussion, next we will formalize the concept of oblivious communication and the class of algorithms to be studied.

\subsection{Communication-Oblivious MPC} \label{sec:intro-prob}

Let us first clarify some math conventions in this paper. For an integer $x \ge 1$, the notation $[x]$ denotes the set $\{1, 2, ..., x\}$. Given integers $x > 0$ and $y \ge 0$, let $\binom{x}{y}$ be the number of ways of choosing $y$ elements from a set size $x$; specially, if $y > x$, define $\binom{x}{y}$ $= 0$. We use double-curly braces to represent multi-sets, e.g., $\ldb 1, 1, 1, 2, 2, 3\rdb$ is a multi-set with 6 elements. Every $\log(\cdot)$ has base $2$. The notation $\poly(n)$ denotes the class of functions polynomial in $n$.


\extraspacing {\bf The MPC Model.} Our analysis will be under the {\em massively parallel computation} (MPC) model, which has been extensively adopted in the database literature, as will be surveyed in Section~\ref{sec:intro-prev}.

\vgap

In this model, $p$ share-nothing machines are connected via a high-speed network. Initially, the input data is distributed across the $p$ machines. An algorithm then executes in {\em rounds}, each consisting of two phases: in the first phase, every machine performs computation on its local data, while in the second phase, the machines exchange messages via the network. Importantly, if machine $i \in [p]$ plans to send machine $j \in [p]$ a message $M[i,j]$ (in the second phase), it must prepare the message in the first phase. This restriction prevents machine $i$ from, for example, deciding what to send based on the messages received {\em during} the second phase. The {\em load of a round} is the maximum number of words communicated (sent and received combined) by one machine in that round. The algorithm's performance is measured by two metrics: (i) the total number of rounds executed, and (ii) the maximum load of all rounds, which is termed as the algorithm's (overall) {\em load}.

\vgap

Randomization is modeled by introducing a sequence of random bits, which is agreed upon by all machines. Such an agreement can be reached before receiving the input and therefore does not require communication during the algorithm's execution. In the first phase of a round, each machine can guide its local computation according to the random-bit sequence. When a random event is said to occur ``with high probability'' (w.h.p.), we require that the probability must be at least $1-1/p^c$ where $c$ can be an arbitrarily large constant chosen before seeing the input data.

\vgap

The value of $N$ is commonly assumed to be significantly larger than $p$, which typically means $N \ge p^c$ where $c$ is a constant dependent on the problem studied.

\extraspacing {\bf Communication-Oblivious Algorithms.} We will now formally define the notion of communication obliviousness, building upon a similar formulation in \cite{ccls20}.

\vgap

We consider that an input $\I$ to the underlying problem is a set of elements, each represented with a constant number of words. The {\em problem size} $N$ of $\I$ is defined as the set size $|\I|$. Given a specific value of $N \ge p$, let $\II(N)$ be the collection of all possible inputs with the same problem size $N$. Fix an arbitrary input $\I \in \II(N)$. As mentioned, initially the $N$ elements of $\I$ are stored across the $p$ machines. We say that a partition $\pi = (\I_1, \I_2, ..., \I_p)$ of $\I$ --- that is, $\I_1, ..., \I_p$ are mutually disjoint and their union is $\I$ --- is {\em legal} if assigning $\I_i$ to machine $i$ for each $i \in [p]$ is permitted by the underlying problem.

\vgap

Consider an arbitrary legal partition $\pi = (\I_1, \I_2, ..., \I_p)$ of $\I$ $\in$ $\II(N)$. Suppose that, when executed on $\pi$, an MPC algorithm $\A$ finishes in $\ell$ rounds where, in the $r$-th round ($r \in [\ell]$), machine $i \in [p]$ sends message $\mb{M}_r[i,j]$ to machine $j \in [p]$ (for $i = j$, $\mb{M}_r[i,j]$ is an empty message). Define $\mb{L}_r$ as the $p \times p$ matrix where $\mb{L}_r[i, j]$ equals the length of $\mb{M}_r[i,j]$, measured in the number of words. Every message $\mb{M}_r[i,j]$ is encrypted such that an adversary, who observes the network communication, can only use the sequence %\vspace{-3mm}
\myeqn{
    \sigma = (\mb{L}_1, \mb{L}_2, ..., \mb{L}_\ell) \nn
    %\label{eqn:intro:pattern}
}
to infer about $\I$. We refer to $\sigma$ as the {\em communication pattern} of $\A$ when executed on $\pi$. For a randomized algorithm $\A$, the sequence $\sigma$ may be a random variable.

\vgap

The algorithm $\A$ is {\em communication oblivious} under a specific problem size $N$ if its communication pattern $\sigma$ follows exactly the same probabilistic distribution, regardless of the choice of $\I \in \II(N)$ and the legal partition $\pi$ of $\I$. We say that $\A$ is a {\em single-round algorithm} if $\ell = 1$ for any choice of $\I$ and $\pi$; otherwise, $\A$ is a {\em multi-round algorithm}.

\subsection{Our Contributions} \label{sec:intro:ours}

Our primary objective is to discern the inherent connections between the traditional MPC model and its communication-oblivious counterpart. The core is to understand whether obliviousness would necessitate a provably higher load than the traditional, non-oblivious, MPC model. Next, we provide an overview of our findings.

\extraspacing {\bf Single-Round Hashing.} Our first contribution is a hardness result proving that {\em hashing} --- a crucial building brick deployed extensively by MPC algorithms (in both theory and practice) --- is significantly more expensive under communication obliviousness as far as single-round algorithms are concerned.

\vgap

To pinpoint the source of hardness, we introduce a problem named {\em skew-free gathering}. Fix arbitrary integers $N$ and $p$ such that $N$ is a multiple of $p$, and $p \ge t$ where $t$ is a constant integer at least 2. The input collection $\II(N)$ has only a single input $\I$: this is a set of $N$ elements, each of which --- denoted as $e$ --- is associated with an integer $\key(e)$, referred to as the {\em key} of $e$. The set of keys fulfills two conditions:
\myitems{
    \item There is one special key that is possessed by $t \cdot (N/p)$ elements, called the {\em special elements}.

    \item Every other key is possessed by exactly one element.
}
A partition $\pi = (\I_1, ..., \I_p)$ of $\I$ is {\em legal} if (i) $|\I_i| = N/p$ for all $i \in [p]$, and (ii)
the $t \cdot N/p$ special elements are placed on $t$ distinct machines, i.e., there are $t$ different machine ids $j \in [p]$ such that $\I_j$ contains $N/p$ special elements. The goal of an algorithm is to move all the special elements to one (arbitrary) machine.

\vgap

In the conventional MPC model, the problem can be solved trivially with load $O(N/p)$: simply ask the $t$ machines where the special elements are stored initially to send their data to a common machine. In contrast, we prove that if a single-round communication-oblivious algorithm is required to succeed with probability at least $2/3$, its load must be at least $\Omega(N / p^{1/t})$ with a constant probability. As $t$ increases, the load approaches $\Omega(N)$ up to a factor sub-polynomial in $p$. More precisely, there does not exist any constant $\eps > 0$ such that one can design an algorithm promising a load of $O(N/p^\eps)$ with a constant probability for all $t = O(1)$. We also show that the lower bound $\Omega(N / p^{1/t})$ can be matched by a deterministic algorithm.

\vgap

Let us now turn to {\em skew-free hashing}, where each input in $\II(N)$ is a set $\I$ of $N$ elements, each with an integer key (computed by some hash function). It is guaranteed that every possible key is possessed by $O(N/p)$ elements of $\I$ (hence, skew ``free''). Initially, each machine receives at most $\ceil{N/p}$ elements. The goal is to move all the elements of the same key to a common machine. In the traditional MPC model, the problem can be settled in one round using load $\tO(N/p)$ w.h.p.\ \cite{bks14,bks17c}, where $\tO(\cdot)$ hides a $\polylog p$ factor. As skew-free hashing generalizes skew-free gathering\footnote{Recall that skew-free gathering has the additional constraint that all the elements except for the special ones need to have distinct keys.}, our lower bound on the latter indicates that oblivious skew-free hashing demands a load of nearly $\Omega(N)$, thus creating a huge separation from the traditional model. 

%It is worth pointing out that $O(N)$ is the worst possible cost under MPC and can be trivially attained --- obliviously --- by having all machines send their elements to the first machine.

\extraspacing {\bf Single-Round Joins.} The hardness of skew-free hashing under communication obliviousness is alarming because the existing one-round join algorithms \cite{assu13,au11,bks13,bks14,bks17c,kbs16} in the (conventional) MPC model depend on a method named\footnote{Also known as the {\em hyper-cube} method.} \texttt{Share} \cite{au11,bks17c}, which uses hashing as a primitive. Because the community has come very close to resolving the one-round communication complexity of (natural) joins \cite{kbs16}, it would be a huge pity if the topic had to be re-opened in the name of obliviousness.

\vgap

Our second contribution is to show that, fortunately, joins' single-round communication complexities are not affected by obliviousness! To explain, let us first formalize the {\em join problem} under the oblivious setting. Fix an arbitrary integer $N \ge 1$ as the problem size.  Let $\att$ be a finite set, where each element is called an {\em attribute}. Consider a non-empty set $\X \subseteq \att$ of attributes. A {\em tuple} over $\X$ is a function $\bm{u}: \X \rightarrow [N]$. For any non-empty subset $\Y \subseteq \X$, we define the {\em projection} of $\bm{u}$ onto $\Y$, denoted as $\bm{u}[\Y]$, as the tuple $\bm{v}$ over $\Y$ that satisfies $\bm{v}(Y) = \bm{u}(Y)$ for every attribute $Y \in \Y$. A {\em relation} $R$ is a set of tuples over the same set $\Z$ of attributes; we refer to $\Z$ as the {\em schema} of $R$ and represent it as $\schema(R)$. A {\em join} is a set $\Q$ of at least two relations such that $\sum_{R \in \Q} |R| = N$ (i.e., $N$ tuples in total in all the input relations). Define $\schema(\Q) = \bigcup_{R \in \Q} \schema(R)$. The result of $\Q$ is a relation over $\schema(\Q)$, formalized as:
\myeqn{
\join(\Q) = \set{\textrm{tuple $\bm{u}$ over $\schema(\Q)$} \mid \forall R \in \Q: \bm{u}[\schema(R)] \in R}. \nn
}
The {\em schema graph} of $\Q$ is the hypergraph $\G = (\V, \E)$ where
\myeqn{
    \V = \schema(\Q) \text{ and }
    \E = \ldb \schema(R) \mid R \in \Q \rdb. \nn
}
Note that $\E$ is a multi-set because some relations in $\Q$ may have the same schema. Focusing on ``data complexity'', we consider only joins whose schema graphs have constant sizes.

%The quantity $\max_{e \in \E} |e|$ is defined as the {\em arity} of $\G$.

\vgap

Given a hypergraph $\G = (\V, \E)$ and an integer $N \ge 1$ that is a multiple of $p$, we define the class of {\em $(\G, N)$-joins} as the set of joins that have schema graph $\G$ and problem size $N$. In the terminology of communication obliviousness, the collection $\II(N)$ comprises all the $(\G, N)$-joins of the same $\G$. Specifically, each input $\I$ of $\II(N)$ is the set of tuples in the relations of a $(\G, N)$-join $\Q$. We define a partition $\pi = (\I_1, ..., \I_p)$ of $\I$ to be {\em legal} if each $\I_i$, $i \in [p]$, consists of $N/p$ tuples. Given $\pi$, an algorithm $\A$ must output each tuple of $\join(\Q)$ on at least one machine.

\vgap

We show that any $(\G, N)$-join $\Q$ can be settled --- obliviously --- in a single round with load $\tO(N/p^{1/\psi})$ w.h.p.\ where $\psi \ge 1$ is the {\em edge quasi-packing number} \cite{kbs16} of $\G$; see Appendix~\ref{app:hypergraph} for the definition of $\psi$. The load complexity asymptotically matches that of the state-of-the-art single-round algorithm \cite{kbs16} in the traditional MPC model. Like \cite{kbs16}, our algorithm assumes that (i) $N \ge p^c$ where $c$ is a constant dependent on $\G$, and (ii) the machines are aware of certain ``heavy'' values that appear frequently in the relations of $\Q$.   

\vgap

Our algorithm applies hashing in a more refined manner compared to \cite{kbs16}. The load $\tO(N/p^{1/\psi})$ stands as an intriguing contrast to our $\Omega(N)$ lower bound (up to a factor sub-polynomial in $p$) on skew-free hashing. This serves as encouraging evidence that hashing can still be powerful in designing single-round algorithms under communication obliviousness. For the join problem, our deployment of hashing circumvents the pitfall of skew-free gathering. Specifically, the deployment ensures that, for each tuple $\bm{u}$ in the join result, the $|\Q|$ tuples in $\set{\bm{u}[\schema(R)] \mid R \in \Q}$ can be transferred to a common machine (which can differ for each $\bm{u}$). Subtly, this is different from sending all tuples having an identical value on an attribute to the same machine, as required by skew-free gathering.

\extraspacing {\bf Round-Doubling Compilation.} The above discussion has focused on single-round algorithms. Our next contribution is a {\em round compilation} method for multi-round algorithms that translates a traditional MPC algorithm $\A$ into the communication-oblivious model while preserving the number of rounds of $\A$ by a factor of 2 and its load by an arbitrarily small constant factor $\delta > 0$. Specifically, if $\A$ performs at most $\ell = \poly(p)$ rounds and, w.h.p., incurs a load of at most $L$, our method yields an oblivious algorithm $\A'$ that computes the same information as $\A$ in $2\ell$ rounds with a load $(1+\delta) L$. Our compilation succeeds w.h.p.\ as long as $L = \Omega(p \log p)$, a condition satisfied by nearly all the existing MPC algorithms we are aware of.

\vgap

We also study how much improvement can still be expected over our compilation method. First, it is clear that the blow-up factor of 2 in the round number cannot be improved in general. As discussed before, for skew-free gathering, it is easy to achieve load $O(N/p)$ in one round in the traditional MPC model, but under communication obliviousness, every single-round algorithm must entail a load of nearly $\Omega(N)$ with a constant probability. Thus, the remaining question is whether the condition $L = \Omega(p \log p)$ can be significantly relaxed. Our last contribution is to answer the question in the negative by establishing a hardness result on the following {\em token passing problem}.

\vgap

Fix arbitrary integers $N$ and $p$ such that $p \ge 2$ and $N = p \cdot \ceil{p^{1-\eps}}$, where the constant $\eps$ satisfies $0 < \eps < 1$. The input collection $\II(N)$ has only a single input $\I$, which is a set of $N$ elements among which there are $N / p = \ceil{p^{1-\eps}}$ special elements called {\em tokens}. In addition to $\I$, the problem takes another two parameters: $x$ and $y$, which are distinct integers in $[p]$. A partition $\pi = (\I_1, ..., \I_p)$ of $\I$ is {\em legal} if (i) $|I_i| = N/p$ for all $i \in [p]$, and (ii) the $N/p$ tokens are all placed on machine $x$ (i.e., $I_x$ consists of nothing but tokens). The objective of the problem is to move all the tokens to machine $y$.\footnote{Unlike skew-free gathering where all the special elements can be moved to an arbitrary machine, here all the tokens must be sent to machine $y$.}

\vgap

In the traditional model, the problem can be trivially solved in one round with load $L = O(p^{1-\eps})$: simply ask machine $x$ to send all the tokens to machine $y$. We prove that, if a two-round communication oblivious algorithm is required to succeed with probability at least 2/3, its load must be $\Omega(p^{1-\eps/2})$ with a constant probability. The polynomial gap between $\Omega(p^{1-\eps/2})$ and $O(p^{1-\eps})$ indicates that the condition $L = \Omega(p \log p)$ of our round-doubling compilation  is tight up to a factor sub-polynomial in $p$. In fact, this remains true even if the blow-up factor $1+\delta$ in load preservation is increased to $\polylog p$. 

\subsection{Previous Results and Relevance to Ours} \label{sec:intro-prev}

Closely related to our work is a compilation method by Chan et al.\ \cite{ccls20}. Our compilation draws inspiration from theirs but makes several improvements. First, the approach of \cite{ccls20} was designed for the ``SODA MPC'' model \cite{ksv10}, which resembles the model in Section~\ref{sec:intro-prob} but does away the notion of ``load''. In that model, each machine is equipped with $s = \Omega(N/p)$ words of memory, and the amount of communication is not a main concern as long as each machine receives no more than $s$ words in each round from all other machines combined. The analysis of \cite{ccls20} was carried out under the condition of $s = N^\eps$ for some constant $\eps > 0$. When translated into our scenario, the analysis fails to capture the regime where $L \ll s$. Second, our compilation is substantially simpler, restores the clarity for the approach underneath \cite{ccls20} in the scenario where $L = \Omega(p \log p)$, and explicitly determines the blow-up factors $1+\delta$ and 2 (for preserving the load and number of rounds, respectively)\footnote{In \cite{ccls20}, these factors were hidden in big-$O$.}. Finally, our discussion (through the token passing problem) on the condition that $L$ needs to satisfy to enable round-doubling load-preserving compilation is new.

\vgap

It should be further noted that the study in \cite{ccls20} does not address one-round MPC algorithms. Through our round compilation method, one can see that a primary distinction between the conventional MPC model and its oblivious counterpart lies in the realm of one-round algorithms. Exploring fundamental problems that can characterize this difference represents an interesting direction. Our work can be seen as a step in that direction.

\vgap

As mentioned, the existing single-round algorithms for join processing \cite{assu13,au11,bks13,bks14,bks17c,kbs16} rely crucially on the \texttt{Share} method \cite{au11,bks17c}. Given a join $\Q$, \texttt{Share} assigns a hash function $h_X(\cdot)$ to each attribute $X \in \schema(\Q)$. For each tuple $\bm{u}$ in a relation $R \in \Q$, the machine initially storing $\bm{u}$ transmits it to a set of machines determined by the hash values in $\set{h_X(\bm{u}(X)) \mid X \in \schema(R)}$. Beame et al.\ \cite{bks17c} presented a sharp concentration bound on the load of \texttt{Share} when certain skew-free requirements are satisfied. Their bound is useful to our analysis and will be reviewed in Section~\ref{sec:join}. Koutris et al.\ \cite{kbs16} proved that any one-round MPC algorithm (oblivious or not) solving the join problem must incur a load of $\Omega(N/p^{1/\psi})$ with a constant probability, if each machine is aware of only the tuples in its local storage initially.


\vgap

There is an extensive body of literature on multi-round MPC algorithms; see \cite{ajr+17,aksu15,akz19,hyt19,kst22,hcww21,hk20,h21,hy19,qt21,t18b} and the references therein. None of those algorithms was designed to achieve communication obliviousness, but all of them satisfy the condition $L = \Omega(p \log p)$ and, therefore, can be made oblivious by our compilation method. It is worth noting that our compilation also works on the one-round algorithms in \cite{assu13,au11,bks13,bks14,bks17c,kbs16}, but the algorithms after compilation will require two rounds.

\section{Hardness of Skew-Free Gathering} \label{sec:gather}

This section will discuss the skew-free gathering problem defined in Section~\ref{sec:intro-prob} and serves as a proof of the paper's first main result:

\begin{theorem} \label{thm:gathering}
    For the skew-free gathering problem parameterized by $t$, any communication-oblivious one-round algorithm succeeding with probability at least $2/3$ must entail a load of $\Omega(N/p^{1/t})$ with at least a constant probability.
    %Furthermore, this is tight: there is a communication oblivious algorithm that always succeeds and guarantees a load of $O(N/p^{1/t})$ deterministically.
\end{theorem}

In Appendix~\ref{app:sf-gathering-alg}, we show that the above lower bound $\Omega(N/p^{1/t})$ can be matched deterministically.

\vgap

To prove Theorem~\ref{thm:gathering}, let us start by identifying a set $\Pi$ of $\binom{p}{t}$ legal partitions of the (sole) input $\I \in \II(N)$ as follows. First, choose a set $Z$ of distinct integers $z_1, z_2, ..., z_t$ in $[p]$; we will refer to $Z$ as the {\em seed machine set}. Second, place $N/p$ special elements on machine $z_j$ for each $j \in [t]$, and place $N/p$ innocuous elements on each machine $i \in [p] \setminus Z$, ensuring that every  element is on exactly one machine. This defines a legal partition in $\Pi$. As there are $\binom{p}{t}$ ways to choose $Z$, the size of $\Pi$ is $\binom{p}{t}$. We require a one-round algorithm $\A$ to be oblivious only on $\Pi$, namely, its communication pattern follows the same probabilistic distribution when it is executed on $\pi \in \Pi$. We will argue that the load of $\A$ must still be $\Omega(N/p^{1/t})$ with at least a constant probability if $\A$ succeeds on every $\pi \in \Pi$ with probability at least $2/3$. This is sufficient for validating the lower bound in Theorem~\ref{thm:gathering}.

\extraspacing {\bf Conditional Expected Load.} As $\A$ executes in one-round, its communication pattern can be fully characterized by a single $p \times p$ matrix $\mb{L}$ where $\mb{L}[i,j]$ ($i, j \in [p]$) is the length of the message $M[i,j]$ that machine $i$ sends to machine $j$. The load of $\A$ can now be calculated as
\myeqn{
    \load(\mb{L}) &=&
    \max_{i=1}^p
    \Big(
    \sum_{j=1}^p \mb{L}[i, j] + \mb{L}[j, i]
    \Big).
    \label{eqn:gathering:load-matrix}
}
When $\A$ is randomized, the $M[i,j]$ of all $i, j \in [p]$ are random variables, and hence so are $\mb{L}$ and
$\load(\mb{L})$. Our objective is to prove that $\Pr[\load(\mb{L}) = \Omega(N/p^{1/t})]$ is at least a constant.

\vgap

By communication obliviousness, the distribution of $\mb{L}$ is the same for all the legal partitions in $\Pi$. Define $\L$ as the set of $p \times p$ matrices $\mb{\Lambda}$ satisfying
\myitems{
    \item $\Pr[\mb{L} = \mb{\Lambda}] > 0$, namely, $\A$ exhibits the patten $\mb{L} = \mb{\Lambda}$ with a non-zero probability;
    \item $\max_{i=1}^p \sum_{j=1}^p \mb{\Lambda}[i, j] \le N/p^{1/t}$;
    \item $\max_{i=1}^p \sum_{j=1}^p \mb{\Lambda}[j, i] \le N/p^{1/t}$.
}
Note that when $\mb{L} = \mb{\Lambda}$, {\em every} machine sends/receives at most $N/p^{1/t}$ words. The subsequent discussion will assume
\myeqn{
    \Pr[\mb{L} \in \L] \ge 1/2. \label{eqn:gathering:small-load-1/2}
}
Note that whenever $\mb{L} \notin \L$, some machine communicates at least $N/p^{1/t}$ words and hence $\load(\mb{L}) \ge N/p^{1/t}$. Thus, $\Pr[\mb{L} \notin \L] \ge 1/2$ immediately gives $\Pr[\load(\mb{L}) \ge N/p^{1/t}] \ge 1/2$, thereby verifying the claim in Theorem~\ref{thm:gathering}.

\vgap

We will aim to validate the following lower bound on the expectation of $\load(\mb{L})$ subject to the condition $\mb{L} \in \L$:
\myeqn{
    \expt[\load(\mb{L}) \mid \mb{L} \in \L] &\ge& c \cdot N/p^{1/t}
    \label{eqn:gathering:goal}
}
for some constant $0 < c < 1$. The above implies\footnote{For every $\mb{L} \in \L$, it holds that $\load(\mb{L}) \le 2 N/p^{1/t}$. If \eqref{eqn:gathering:goal-implication} incorrect, then $\expt[\load(\mb{L}) \mid \mb{L} \in \L] <  \fr{2N}{p^{1/t}} \cdot \fr{c}{4} + \fr{c}{2} \fr{N}{p^{1/t}} (1 - \fr{c}{4}) < c N / p^{1/t}$, contradicting \eqref{eqn:gathering:goal}.}
\myeqn{
    \Pr[\load(\mb{L}) \ge (c/2) \cdot N/p^{1/t} \mid \mb{L} \in \L] \ge c/4.
    \label{eqn:gathering:goal-implication}
}
Combining \eqref{eqn:gathering:small-load-1/2} and \eqref{eqn:gathering:goal-implication} yields $\Pr[\load(\mb{L}) \ge \fr{c}{2} N/p^{1/t}] \ge \fr{c}{4} \cdot \fr{1}{2} = c/8$, thus establishing Theorem~\ref{thm:gathering}.

\extraspacing {\bf Heavy Senders.} Suppose that the algorithm $\A$ exhibits a pattern $\mb{L} = \mb{\Lambda} \in \L$. Given a machine $i \in [p]$, we call machine $j \in [p] \setminus \set{i}$ a {\em heavy sender} for machine $i$ under $\mb{\Lambda}$ if $\mb{\Lambda}[j,i] \ge N/p$, namely, machine $j$ sends at least $N/p$ words to machine $i$. Define
\myeqn{
    H(\mb{\Lambda}, i) = \text{number of heavy senders for machine $i$ under $\mb{\Lambda}$}. \label{eqn:gathering:H}
}
It is easy to see that
\myeqn{
    H(\mb{\Lambda}, i) \le p^{1-1/t}
    \label{eqn:gathering:bound-H}
}
because otherwise machine $i$ would receive more than $(N/p) \cdot p^{1-1/t} = N/p^{1/t}$ words, contradicting $\mb{\Lambda} \in \L$.

\vgap

Our analysis will revolve around heavy senders. We will argue that at least one machine $i \in [p]$ satisfies
\myeqn{
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot H(\mb{\Lambda}, i)
    &=&
    \Omega(p^{1-1/t}).
    \label{eqn:gathering:goal2}
}
As $\load(\mb{\Lambda}) \ge (N/p) \cdot H(\mb{\Lambda}, i)$, the above leads to
\myeqn{
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot \load(\mb{\Lambda})
    =
    \Omega\Big(\fr{N}{p} \cdot p^{1-1/t} \Big)
    =
    \Omega(N /p^{1/t}).
    \label{eqn:gathering:goal2-implication}
}
This will prove \eqref{eqn:gathering:goal} because
\myeqn{
    \expt[\load(\mb{L}) \mid \mb{L} \in \L]
    &=&
    \fr{1}{\Pr[\mb{L} \in \L]} \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot \load(\mb{\Lambda})
    \nn
}
which is at least \eqref{eqn:gathering:goal2-implication}.

%as long as $H(\mb{\Lambda}, i) \ge 1$

\extraspacing {\bf Two Types of Legal Partitions.} The rest of the section will focus on proving the correctness of \eqref{eqn:gathering:goal2}. Earlier we have identified a set $\Pi$ of $\binom{p}{t}$ legal partitions, each characterized by a seed machine set $Z = \set{z_1, ..., z_t}$; for convenience, we will use $Z$ to denote the corresponding legal partition when no ambiguity can arise. Now, fix any legal partition $Z$. The algorithm $\A$ succeeds on $Z$ if and only if at least one of the following occurs:
\myitems{
    \item machines $z_1, z_2, ..., z_t$ all send their special elements to a machine $k \in [p] \setminus Z$;
    \item there is some $j \in [t]$ such that machine $z_j$ receives all the special elements from every other machine in $Z$.
}
Echoing the above, given any $Z$ and any matrix $\mb{\Lambda} \in \L$, we define
\myitems{
    \item $X(Z, \mb{\Lambda})$ as an indicator variable, which equals 1 if there exists some $k \in [p] \setminus Z$ such that $\mb{\Lambda}[z_j, k] \ge N/p$ for every $j \in [t]$, or 0 otherwise;

    \item $Y(Z, \mb{\Lambda})$ as an indicator variable, which equals 1 if there exists some $j \in [t]$ such that $\mb{\Lambda}[z_k, z_j] \ge N/p$ for every $k \in [t] \setminus \set{j}$, or 0 otherwise.
}

%

When exhibiting the pattern $\mb{L} = \bm{\Lambda}$, the algorithm $\A$ succeeds on $Z$ only if
\myeqn{
    X(Z, \mb{\Lambda}) + Y(Z, \mb{\Lambda}) \ge 1. \nn
}
We declare:
\myeqn{
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot
    (X(Z, \mb{\Lambda}) + Y(Z, \mb{\Lambda}))
    &\ge&
    1/6.
    \label{eqn:gathering:1/6}
}
To see why, notice that if the above did not hold, then $\A$ would have probability less than $1/6$ succeeding on $Z$ when $\mb{L} \in \L$. Even if $\A$ always succeeds on $Z$ when $\mb{L} \notin \L$, the overall success probability on $Z$ would still be less than $1/6 + \Pr[\mb{L} \notin \L]$, which is less than 2/3 because of \eqref{eqn:gathering:small-load-1/2}. This violates the requirement that $\A$ must succeed with probability at least $2/3$ on $Z$.

\vgap

We will refer to a legal partition $Z$ as
\myitems{
    \item an {\em X-partition} if $\sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot
    X(Z, \mb{\Lambda}) \ge 1/12$, or
    \item an {\em Y-partition} if $\sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot
    Y(Z, \mb{\Lambda}) \ge 1/12$.
}
Note that $Z$ must be classified as at least one of the two types because of \eqref{eqn:gathering:1/6}. Thus, if $\XX$ (resp., $\YY$) represents the set of all X- (resp., Y-) partitions, we have $|\XX| + |\YY| \ge |\Pi| = \binom{p}{t}$. The next lemma relates each type of partitions to heavy-sender machines.

\begin{lemma} \label{lmm:gathering:total-X-Y}
    For any $\mb{\Lambda} \in \L$, it holds that
    \myeqn{
        \sum_{Z \in \XX} X(Z, \mb{\Lambda}) &\le& \sum_{i \in [p]} \binom{H(\mb{\Lambda}, i)}{t} \label{eqn:gathering:total-X-Y:help1} \\
        \sum_{Z \in \YY} Y(Z, \mb{\Lambda}) &\le& \sum_{i \in [p]} \binom{H(\mb{\Lambda}, i)}{t-1}. \label{eqn:gathering:total-X-Y:help2}
    }
\end{lemma}

The reader is reminded that $\binom{x}{y}$ equals 0 if $x < y$. We will prove the first inequality here and the second one in Appendix~\ref{app:proof:thm:gathering}.

\begin{proof}[Proof of \eqref{eqn:gathering:total-X-Y:help1}]
    Our proof adopts a counting argument. Initially, create an empty set $S_i$ for each $i \in [p]$. Process each $Z = \set{z_1, ..., z_t} \in \XX$ as follows. If $X(Z, \mb{\Lambda}) = 0$, do nothing. Otherwise, under the communication pattern $\mb{L} = \bm{\Lambda}$, there is at least one machine $k \in [p] \setminus Z$ that receives $N/p$ special elements from machine $z_j$ for all $j \in [t]$. Thus, machines $z_1, z_2, ..., z_t$ are heavy senders for machine $k$ under $\mb{\Lambda}$. Hence, $Z$ represents a possible way to choose $t$ machines from the $H(\mb{\Lambda}, k)$ heavy senders for machine $k$ under $\mb{\Lambda}$. Accordingly, we add $Z$ to $S_k$. After all the sets $Z \in \XX$ have been processed, the left hand side of \eqref{eqn:gathering:total-X-Y:help1} is bounded by $\sum_{i \in [p]} |S_i|$. On the other hand, for each $i \in [p]$, the size $|S_i|$ cannot exceed the total number of ways of choosing $t$ machines from the $H(\mb{\Lambda}, i)$ heavy senders for machine $i$ under $\mb{\Lambda}$. Inequality \eqref{eqn:gathering:total-X-Y:help1} now follows.
\end{proof}


\vgap

The remainder of the argument proceeds differently depending on the number of $X$-partitions.

\extraspacing {\bf At Least $\fr{1}{2} \binom{p}{t}$ X-Partitions.} That is, $|\XX| \ge \fr{1}{2} \binom{p}{t}$, which, together with the definition of X-partition, yields
\myeqn{
    \sum_{Z \in \XX} \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot
    X(Z, \mb{\Lambda}) \ge \fr{1}{24} \binom{p}{t}.
    \label{eqn:gathering:X-help1}
}
We can now derive
\myeqn{
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot \sum_{i \in [p]} H(\mb{\Lambda},i)^t
    &\ge&
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot \sum_{i \in [p]} \binom{H(\mb{\Lambda},i)}{t}
    \nn \\
    \explain{by \eqref{eqn:gathering:total-X-Y:help1}}
    &\ge&
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot \sum_{Z \in \XX} X(Z, \mb{\Lambda})
    \nn \\
    &=&
    \sum_{\mb{\Lambda} \in \L} \sum_{Z \in \XX} \Pr[\mb{L} = \mb{\Lambda}] \cdot  X(Z, \mb{\Lambda})
    \nn \\
    \explain{by \eqref{eqn:gathering:X-help1}}
    &\ge&
    \fr{1}{24} \binom{p}{t}.
    \label{eqn:gathering:X-help2}
}
This gives:
\myeqn{
    %\text{LHS of \eqref{eqn:gathering:goal2}}
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \sum_{i \in [p]} H(\mb{\Lambda},i)
    &=&
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot \sum_{i \in [p]} \fr{H(\mb{\Lambda},i)^t}{H(\mb{\Lambda},i)^{t-1}}
    \nn \\
    \explain{by \eqref{eqn:gathering:bound-H}}
    &\ge&
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot \sum_{i \in [p]} \fr{H(\mb{\Lambda},i)^t}{p^{\fr{(t-1)^2}{t}}}
    \nn \\
    \explain{by \eqref{eqn:gathering:X-help2}}
    &\ge&
    \fr{1}{24} \binom{p}{t} / p^{\fr{(t-1)^2}{t}}
    \nn \\
    &=&
    \Omega\Big(
        \fr{p^t}{p^{(t-1)^2/t}}
    \Big)
    =
    \Omega\Big(p^{2-1/t} \Big).
    \nn
}
Therefore, at least one $i \in [p]$ satisfies $\sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot H(\mb{\Lambda},i) = \Omega(p^{2-1/t} / p ) = \Omega (p^{1-1/t})$, as claimed in \eqref{eqn:gathering:goal2}.

\extraspacing {\bf At most $\fr{1}{2} \binom{p}{t}$ X-Partitions.} Because $|\XX| + |\YY| \ge \binom{p}{t}$, the fact $|\XX| \le \binom{p}{t}/2$ indicates $|\YY| \ge \binom{p}{t}/2$. Following a derivation similar to the previous case, we can show that at least one $i \in [p]$ satisfies \eqref{eqn:gathering:goal2}. With the details presented in Appendix~\ref{app:proof:thm:gathering}, we now conclude the proof of Theorem~\ref{thm:gathering}.

\section{Single-Round Oblivious Joins} \label{sec:join}

We now turn our attention to the join problem defined in Section~\ref{sec:intro:ours}. Let $\Q$ be the input $(\G, N)$-join. Given a value $x \in [N]$, we call it
\myitems{
    \item {\em heavy} if at least $N / p^2$ tuples of a relation have value $x$ under an attribute, or formally, $\exists R \in \Q, X \in \schema(R)$ s.t. $R$ has at least $N / p^2$ tuples $\bm{u}$ with $\bm{u}(X) = x$;

    \item or {\em light}, otherwise.
} 
There can be $O(p^2)$ heavy values in total. We make the {\em heavy-statistics assumption} that each machine knows all the heavy values. For this purpose, each legal partition (of the tuples in the relations of $\Q$; see Section~\ref{sec:intro:ours}) is augmented with an ``information field'' for each machine, which contains all the heavy values. To prevent an adversary from distinguishing the $(\G, N)$ joins by inspecting the information field's length, the field is always padded with dummy data to the same length of $O(p^2)$ words.

\vgap 

We will establish our second main result:

\begin{theorem} \label{thm:join}
    Consider any constant-size hypergraph $\G = (\V, \E)$. Let $N$ be a multiple of $p$ satisfying $N \ge p^{1 + 2\alpha + 1/\psi}$ where $\alpha = \max_{e \in \E} |e|$ (called the arity of $\G$) and $\psi$ is the edge quasi-packing number of $\G$. Under the heavy-statistics assumption, there is a communication oblivious algorithm that,  given any $(\G, N)$-join $\Q$, operates in a single round with load $\tO(N / p^{1/\psi})$ and computes $\join(\Q)$ w.h.p..
\end{theorem}

Our result can be compared to an algorithm of \cite{kbs16}, which has load $\tO(N / p^{1/\psi})$ w.h.p.\ but is not communication oblivious. That algorithm also makes the heavy-statistics assumption although their frequency threshold of a heavy value is $N/p$, rather than $N/p^2$ in our context.

%$\alpha = \max_{e \in \E} |e|$ (this is the arity of $\G$), and

\vgap 

The rest of the section serves as a proof of Theorem~\ref{thm:join}. We will first review the \ttt{Share} method \cite{au11,bks17c} in Section~\ref{sec:join-share}. After that, we will present our join algorithm in Sections~\ref{sec:join-alg1} and \ref{sec:join-alg2} to attain the performance guarantees claimed.

%and analyzing its behavior in Section~\ref{sec:join-analysis}.

\subsection{The Share Method} \label{sec:join-share}

Let $R^*$ be a relation with $\schema(R^*) = \set{X_1, X_2, ..., X_r}$, and $M$ be an integer satisfying $|R^*| \le M \le N$. Recall that, as defined in Section~\ref{sec:intro:ours}, each tuple $\bm{u} \in R^*$ is a function from $\schema(R^*)$ to $[N]$. For any $\Y \subseteq \schema(R^*)$, define:
%{\em the degree of $R^*$ on $\Y$} as
\myeqn{
    \deg_\Y(R^*) &=& \max_{\text{tuple $\bm{u}$ over $\Y$}} \big|\set{\bm{v} \in R^* \mid \bm{v}[\Y] = \bm{u}} \big| \label{eqn:join-degree}
}
that is, the maximum number of tuples in $R^*$ having the same projection on $\Y$. Suppose that we assign to each attribute $X_i$, $i \in [r]$, a positive integer $s_i$ --- called the {\em share} of $X_i$ --- satisfying the following {\em skew-free condition}:
\myeqn{
    \text{for any non-empty $\Y \subseteq \schema(R^*)$, $\deg_\Y(R^*) \le \fr{M}{{\prod_{i \in [r]: X_i \in \Y} s_i}}$}.
    \label{eqn:share-skewfree}
}
Fix any constant $\alpha \ge r$. For each $i \in [r]$, choose independently a {\em perfectly random hash function} $h_i: [\alpha N] \rightarrow [s_i]$, that is, $h_i$ is uniformly sampled from the $(s_i)^{\alpha N}$ possible functions from $[\alpha N]$ to $[s_i]$. Given $\bm{b} = (b_1, b_2, ..., b_r)$ $\in [s_1] \times [s_2] \times ... \times [s_r]$, define
\myeqn{
    \text{\em bin $\bm{b}$}
    &=&
    \set{\bm{u} \in R^* \mid \forall i \in [r], h_i(\bm{u}(X_i)) = b_i}.
    \label{eqn:join-share:bin}
}
Note that there are in total $\prod_{i=1}^r s_i$ bins. The following concentration bound is due to Beame et al.\ \cite{bks17c}:

\begin{lemma} \label{lmm:share}
    %[\cite{bks17c}]
    For any constant $c > 0$, it holds with probability at least $1 - (1/p^*)^c$ that the sizes of all the bins are bounded by $O((\log p^* / \log\log p^*)^r \cdot M / p^*)$, where $p^* = \prod_{i=1}^r s_i$. The constant in the big-$O$ depends on $c$.
\end{lemma}

Strictly speaking, Beame et al.\ \cite{bks17c} proved the lemma only for the case where $M = |R^*|$, which is not enough for our purposes. Fortunately, it is not difficult to extend their result to any $M \in [|R^*|, N]$, as shown in Appendix~\ref{app:proof:thm:join}.

\subsection{A Non-Oblivious Join Algorithm} \label{sec:join-alg1}

This subsection will present an algorithm, which is {\em not} communication oblivious, to compute the result of a $(\G, N)$-join $\Q$ with load $\tO(N / p^{1/\psi})$ w.h.p.. The reason for describing this non-oblivious version is to allow the reader to draw a direct comparison with the solution of \cite{kbs16}. As will be clear, a key new idea is to apply the concentration bound of Lemma~\ref{lmm:share} locally on each machine, rather than globally on the entire join (as was done in \cite{kbs16}). This in turn requires us to decrease the ``heavy threshold'' from $N/p$ to $N/p^2$. Our algorithm ensures a new property (to be given in Lemma~\ref{lmm:join-comm}), which makes it easy to make the algorithm communication oblivious, as we do in the next subsection.

\vgap

Given a relation $R \in \Q$ and machine id $i \in [p]$, let $R^{(i)}$ be the set of tuples of $R$ that are initially stored on machine $i$. Define $R^{(i,\heavy)}$ as the set of tuples $\bm{u} \in R^{(i)}$ using {\em only} heavy values. We know
\myeqn{
    |R^{(i,\heavy)}| &=& O(p^{2\alpha}) \nn
    %\label{eqn:join-size-R-i-heavy}
}
because $R^{(i)}$ has at most $\alpha$ attributes (recall that $\alpha$ is the arity of the hypergraph $\G$), and there can be $O(p^2)$ heavy values.

\vgap

Before proceeding, the reader should be familiarized with the content of Appendix~\ref{app:hypergraph}, in particular, the notions of ``residual graph'' and ``fractional vertex cover''. Next, let us take an arbitrary proper subset $\Z \subset \V$ and identify an optimal fractional vertex cover $w_\Z:$ $\V$ $\setminus$ $\Z \rightarrow [0, 1]$ of the residual graph $\G_\Z = (\V_\Z, \E_\Z)$. Define
\myeqn{
    \tau(\G_\Z) &=& \sum_{X \in \V \setminus \Z} w_\Z(X).
    \label{eqn:join-tau_Z}
}
To each attribute $X \in \V$, we assign a positive integer $s_X(\Z)$ --- the share of $X$ for $G_\Z$ --- as follows:
\myitems{
    \item if $X \in \Z$, then $s_X(\Z) = 1$;
    \item otherwise, $s_X(\Z) = \lf p^{w_\Z(X) / \tau(\G_\Z)} \rf$.
}
For any non-empty subset $\Y \subseteq \V$, we show in Appendix~\ref{app:proof:thm:join}:
\myeqn{
    \prod_{X \in \Y} s_X(\Z)
    \le
    p.
    \label{eqn:join-help1}
}
Furthermore, for any relation $R \in \Q$, we show again in Appendix~\ref{app:proof:thm:join}:
\myeqn{
    \prod_{X \in \schema(R)} s_X(\Z) &=& \Omega\Big(p^{1/\tau(G_\Z)} \Big).
    \label{eqn:join-help2}
}

Henceforth, set
\myeqn{
    M = N/p. \label{eqn:join-M}
}
For each $i \in [p]$ and each relation $R \in \Q$ satisfying $\schema(R) \setminus \Z \ne \emptyset$, we define $R^{(i,\Z)}$ as the set of tuples $\bm{u} \in R^{(i)}$ such that
\myitems{
    \item $\bm{u}(X)$ is heavy for every attribute $X \in \schema(R) \cap \Z$;
    \item $\bm{u}(X)$ is light for every attribute $X \notin \schema(R) \cap \Z$.
}
For any non-empty subset $\Y \subseteq \schema(R)$, let us observe
\myeqn{
    \deg_\Y (R^{(i,\Z)})
    \le \fr{M}{\prod_{X \in \Y} s_X(\Z)}.
    \label{eqn:join-alg-skewfree}
}
Indeed, if $\Y \subseteq \Z$, then $\prod_{X \in \Y} s_X(\Z) = 1$ in which case \eqref{eqn:join-alg-skewfree} holds because $\deg_\Y (R^{(i,\Z)}) \le |R^{(i)}| \le N/p = M$. Otherwise, identify an arbitrary attribute $X \in \Y \setminus \Z$. Then, we can derive
\myeqn{
    \deg_\Y (R^{(i,\Z)}) \le \deg_{\set{X}} (R^{(i,\Z)}) \le \deg_{\set{X}} (R^{(i)}) \le N/p^2 \nn
}
where the last inequality is due to the definition of light value. By \eqref{eqn:join-help1}, $N/p^2 \le M / \prod_{X \in \Y} s_X(\Z)$, from which the correctness of \eqref{eqn:join-alg-skewfree} follows. We can therefore conclude that $R^{(i,\Z)}$ fulfills the skew-free condition prescribed in \eqref{eqn:share-skewfree}.

\vgap

We now elaborate on the join algorithm. Before receiving the $(\G,N)$-join $\Q$, we perform some preprocessing for each proper subset $\Z \subset \V$. First, obtain $s_X(\Z)$ --- the share of $X$ for $G_\Z$ --- for every attribute $X \in \V$ as explained earlier. For each $X \in \V$, independently pick a perfectly random hash function $h_{\Z, X}: [\alpha N] \rightarrow [s_X(\Z)]$ (recall that $\alpha$ is the arity of $\G$). Store a copy of all these functions on every machine. The cartesian product of $\bigtimes_{X \in \V} [s_X(\Z)]$ has a size of $\prod_{X \in \V} s_X(\Z)$, which is at most $p$ (see \eqref{eqn:join-help1}). Each element in the cartesian product can be regarded as a tuple $\bm{b}_\Z$, where $\bm{b}_\Z(X)$ is an integer in $[s_X(\Z)]$ for each $X \in \V$. We assign a distinct machine to each possible $\bm{b}_\Z$ lexicographically: machine $i \in [p]$ assigned to the $\bm{b}_\Z$ that ranks, in lexicographic order, the $i$-th among all the elements of $\bigtimes_{X \in \V} [s_X(\Z)]$. Note that some machines (precisely, $p - \prod_{X \in \V} s_X(\Z)$ machines) are not assigned to any cartesian product elements.

\vgap

After the $(\G,N)$-join $\Q$ has been given, each machine --- say the one with id $i \in [p]$ --- carries out the following steps in parallel:

\myitems{
    \item {{\bf S1:}} Broadcast $R^{(i,\heavy)}$.

    \vgap

    \item {{\bf S2:}} For each proper subset $\Z \subset \V$ and every relation $R \in \Q$ satisfying $\schema(R) \setminus \Z \ne \emptyset$, the machine examines every tuple $\bm{u} \in R^{(i,\Z)}$. For each attribute $X \in \schema(R)$, the machine computes the hash value $h_{\Z,X}(\bm{u}(X))$. Then, it sends $\bm{u}$ to every machine $\bm{b}_\Z \in \bigtimes_{X \in \V} [s_X(\Z)]$ satisfying $\bm{b}_\Z(X) = h_{\Z,X}(\bm{u}(X))$ for all $X \in \schema(R)$.
    %The number of such machines is $\prod_{X \in \V \setminus \schema(R)} s_X(\Z)$, which is $O(p^{1-1/\tau(\G_\Z)})$ by \eqref{eqn:join-help3}.
}

The above steps require one round of communication. Then, every machine joins all the tuples received. To prove  correctness, consider any tuple $\bm{u} \in \join(\Q)$. If $\bm{u}$ uses a heavy value on every attribute, then it is produced by all machines due to Step {\bf S1}. Otherwise, suppose that $\bm{u}$ uses heavy values only on those attributes of $\Z$ for some $\Z \subset \V$. Then, due to Step {\bf S2}, it must be produced at the machine corresponding to the element $\bm{b}_\Z \in \bigtimes_{X \in \V} [s_X(\Z)]$ where $\bm{b}_\Z(X) = h_{\Z,X}(\bm{u}(X))$ for all $X \in \V$.
 
\vgap 

We will prove that the load incurred is $O(p^{2\alpha + 1}) + \tO(N/p^{1/\psi})$ w.h.p., which is $\tO(N/p^{1/\psi})$ for $N \ge p^{1 + 2\alpha + 1/\psi}$. Clearly, Step {\bf S1} entails load $O(p^{2\alpha + 1})$. Next, we will show that, for any particular $\Z \subset \V$, {\bf S2} generates a load of $\tO(N / p^{1/\tau(\G_\Z)})$ w.h.p., where $\tau(\G_\Z)$ is given in \eqref{eqn:join-tau_Z}. It will then follow that the overall load of {\bf S2} is $\tO(N/p^{1/\psi})$ because $\psi$ equals the maximum $\tau(\G_\Z)$ of all $\Z \subset \V$ (see \eqref{eqn:hypergraph:psi}) Appendix~\ref{app:hypergraph}) and $\V$ has a constant number of subsets.

\vgap 

The lemma below characterizes the behavior of our algorithm.

\begin{lemma} \label{lmm:join-comm}
    Consider an arbitrary proper subset $\Z \subset \V$. The following statement holds w.h.p.: for any relation $R \in \Q$ and distinct machine ids $i, j \in [p]$, machine $i$ sends $\tO(M / p^{1/\tau(\G_\Z)})$ tuples in $R^{(i,\Z)}$ to machine $j$.
\end{lemma}

\begin{proof}
     As explained previously, relation $R^{(i,\Z)}$ satisfies the skew-free condition \eqref{eqn:join-alg-skewfree} with respect to the shares $\set{s_X(\Z) \mid X \in \V}$. Set $p^* = \prod_{X \in \schema(R)} s_X(\Z)$. As $|R^{(i,\Z)}| \le M \le N$, Lemma~\ref{lmm:share} asserts that the following event occurs w.h.p.:
     
     \minipg{0.9\linewidth}{
        $\tO(\fr{M}{p^*})$ tuples $\bm{u} \in R^{(i,\Z)}$ are hashed to each element (a.k.a., bin) $\bm{b}$ in the cartesian product $\bigtimes_{X \in \schema(R)} [s_X(\Z)]$, namely, $\bm{b}(X) = h_{\Z,X}(\bm{u}(X))$ for all $X \in \schema(R)$.
     }
     
     \noindent Under the above event, $\tO(M/p^*)$ tuples $\bm{u} \in R^{(i,\Z)}$ are hashed to each element $\bm{b}_\Z \in \bigtimes_{X \in \V} [s_X(\Z)]$, i.e., such a tuple $\bm{u}$ satisfies $\bm{b}_\Z(X) = h_{\Z,X}(\bm{u}(X))$ for all $X \in \schema(R)$. Hence, if the machine $j$ stated in the lemma is assigned to an element in  $\bigtimes_{X \in \V} [s_X(\Z)]$, then machine $i$ sends $\tO(M/p^*)$ tuples of $R^{(i,\Z)}$ to machine $j$; otherwise, machine $i$ sends no tuple of $R^{(i,\Z)}$ to machine $j$.
     
     \vgap 
     
     The lemma now follows from $p^* = \Omega(p^{1/\tau(\G_\Z)})$ (see \eqref{eqn:join-help2}).
\end{proof}

As $\Q$ has $O(1)$ relations, Lemma~\ref{lmm:join-comm} indicates that every machine sends and receives $\tO(p \cdot M / p^{1/\tau(\G_\Z)}) = \tO(N / p^{1/{\tau(\G_\Z)}})$ words in Step {\bf S2} w.h.p.\ in processing the subset $\Z$.

\extraspacing {\bf Remark.} Lemma~\ref{lmm:join-comm} is a feature of our algorithm that is not shared by the algorithm of \cite{kbs16}, but is crucial for turning the algorithm into an oblivious counterpart, as discussed next.

\subsection{Making the Algorithm Oblivious} \label{sec:join-alg2}

Our algorithm in Section~\ref{sec:join-alg1} enjoys a simple communication pattern. It can be modified into a communication-oblivious version by padding enough dummy data to make the length of each message consistent with the ``worst'' case.

\vgap

Let us start with Step {\bf S1}, where machine $i \in [p]$ transmits the same message, which contains $R^{(i,\heavy)}$ for each $R \in \Q$, to every other machine. This message can have a maximum length of $\mit{len}_\mit{heavy}$ $= O(p^{2\alpha})$ words. Whenever this message is shorter than $\mit{len}_\mit{heavy}$, expand it with dummy data into length $\mit{len}_\mit{heavy}$.

\vgap

In Step {\bf S2}, for each proper subset $Z \subset \V$, machine $i \in [p]$ transmits a possibly different message to each other machine $j \in [p]$, whose length is bounded by an integer $\mit{len}_\Z = \tO(M/p^{1/\tau(\G_\Z)})$ w.h.p.\ (Lemma~\ref{lmm:join-comm}). Whenever this message is shorter than $\mit{len}_\Z$ --- including the case of an empty message --- expand it with dummy data into length $\mit{len}_\Z$. However, it is possible for the message to be actually longer than $\mit{len}_\Z$ (this happens when the high-probability event in Lemma~\ref{lmm:join-comm} does not occur). In that scenario, machine $i$ (arbitrarily) trims the message at the length of $\mit{len}_\Z$ and sends the trimmed message to machine $j$ anyway; when this happens, we say that the algorithm {\em errs}.

\vgap

In the above modified algorithm, every machine sends precisely $\mit{len}_\mit{heavy} + \sum_{\Z \subset \V} \mit{len}_\Z$ words to every other machine. It therefore exhibits a deterministic communication pattern for all $(\G,N)$-joins --- this is true regardless of whether the algorithm errs. The load is always $p \cdot (\mit{len}_\mit{heavy} + \sum_{\Z \subset \V} \mit{len}_\Z)$, which is $\tO(N/p^{1/\psi})$ as analyzed in the previous subsection. The algorithm computes the join result correctly if it does not err (an adversary cannot tell whether the algorithm has erred because everything is encrypted). Due to Lemma~\ref{lmm:join-comm}, by choosing constants appropriately, we can reduce the probability that the algorithm errs to at most $1/p^c$ for an arbitrarily large constant $c$. This completes the proof of Theorem~\ref{thm:join}.

\section{Two-Round Compilation} \label{sec:compile}

This section will concentrate on multi-round MPC algorithms, for which our main contribution is a compilation method that establishes the following theorem:

\begin{theorem} \label{thm:compile}
    Fix an arbitrary constant $\delta$ satisfying $0 < \delta < 1$. Let $\A$ be an algorithm under the traditional MPC model that operates within $\ell = \poly(p)$ rounds and entails a load at most $L$ w.h.p.. If $L  = \Omega(p \log p)$ where the hidden constant depends on $\delta$, there is a communication-oblivious algorithm that performs $2\ell$ rounds, requires a load at most $(1 + \delta) L$, and computes the same information as $\A$ on every machine w.h.p..
\end{theorem}

As explained in Section~\ref{sec:intro:ours}, the round blow-up factor 2 in the above theorem is the best possible, regardless of the constant $\delta$. The next theorem indicates that the condition $L = \Omega(p \log p)$ can no longer be relaxed significantly, again as explained in Section~\ref{sec:intro:ours}.

\begin{theorem} \label{thm:token}
    Fix an arbitrary constant $\eps$ satisfying $0 < \eps < 1$. For the token-passing problem (defined in Section~\ref{sec:intro:ours}) parameterized by $\eps$, any communication-oblivious two-round algorithm succeeding with probability at least $2/3$ must demand a load of $\Omega(p^{1-\eps/2})$ with at least a constant probability.
\end{theorem}

We will present the algorithmic procedure of our compilation method in the rest of the section, but defer its analysis to Appendix~\ref{app:compile}. The proof of Theorem~\ref{thm:token} is provided in Appendix~\ref{app:thm:token}.

\extraspacing {\bf Message Routing.} To prove Theorem~\ref{thm:compile}, we will tackle the {\em message routing problem} defined as follows. Suppose that, for each pair of $i, j \in [p]$, machine $i$ needs to send a message $\mb{M}[i,j]$ to machine $j$ (if $i = j$, then $\mb{M}[i,j]$ is an empty message). Denote by $\mb{L}[i,j]$ the length of $\mb{M}[i,j]$ in the number of words. For each $i \in [p]$, we have
\myeqn{
    \sum_{j \in [p]} \mb{L}[i,j] + \mb{L}[j,i] \le L \label{eqn:compile-len-constraint}
}
namely, every machine sends and receives no more than $L$ words in total. We want to design a communication-oblivious algorithm $\A_\mit{route}$ with all the requirements below:
\myitems{
    \item It runs in two rounds.
    \item For distinct machines $i, j \in [p]$, machine $i$ sends (precisely) $L'$ words to machine $j$ in each round, where $L' \le (1+\delta) L/p$.
    \item W.h.p., all the messages are successfully delivered at the end of the second round.
}

Equipped with $\A_\mit{route}$, we can prove Theorem~\ref{thm:compile} as follows. Recall (from Section~\ref{sec:intro-prob}) that each round of $\A$ runs in two phases where in the first phase each machine prepares the messages to be sent out in the second phase. We treat the second phase as an instance of the message routing problem and apply $\A_\mit{route}$ to deliver the messages. W.h.p., at the end of $\A_\mit{route}$, every machine acquires all the information that it would have obtained at the second phase of $\A$, and can thus perform the local computation as demanded by $\A$. Because $\A$ always terminates within $\ell = \poly(p)$ rounds, our compilation succeeds on all its rounds w.h.p..

\extraspacing {\bf Our Algorithm.} Set $\delta' = \ceil{16/\delta}$. We chop each message $\mb{M}[i,j]$ into segments, each of which has $\delta'$ words --- if the last segment is shorter than $\delta'$ words, expand it with dummy words to make its length $\delta'$. The number of segments is $\ceil{\mb{L}[i,j] / \delta'}$. Each segment, referred to as a {\em packet} henceforth, is augmented with three fields: (i) {\em sender}, i.e., the value $i$ (sender machine id), (ii) {\em recipient}, i.e., the value $j$ (recipient machine id), and (iii) {\em sequence number}, i.e., the $t$-th packet of $\mb{M}[i,j]$ has sequence number $t$. Packets may arrive at machine $j$ in an arbitrary order; the sender and sequence-number fields allow the packets to be put back into the original sequence in $\mb{M}[i,j]$. Every packet is thus $\delta' + 3$ words in length.

\vgap

Denote by $P_i$ the set of packets obtained from $\mb{M}[i,1]$, $\mb{M}[i,2]$, ..., $\mb{M}[i,p]$ combined. Let $|P_i|$ be the number of those packets. Then
\myeqn{
    |P_i| \le \sum_{j=1}^p \Big(1 + \fr{\mb{L}[i,j]}{\delta'} \Big) \le p + \fr{L}{\delta'}.
    \label{eqn:compile-P_i-size}
}
where the last inequality used \eqref{eqn:compile-len-constraint}. Set
\myeqn{
    \lambda = \lc \left(1 + \fr{L} {\delta' p}\right)\left(1 + \fr{\delta}{2}\right) \rc.
    \label{eqn:compile-lambda}
}

In the first round, each machine --- say machine $i \in [p]$ --- in parallel carries out the steps below:
\myitems{
    \item To each packet in $P_i$, assign a {\em relay machine} chosen from $[p]$ uniformly at random. Let $P_i(j)$ be the set of packets in $P_i$ assigned to the same relay machine $j \in [p]$. We use $|P_i(j)|$ to represent the number of packets in $P_i(j)$.

    \vgap

    \item For each $j \in [p]$, if $|P_i(j)| < \lambda$, add enough dummy packets to $P_i(j)$ to make the number of packets therein exactly $\lambda$. If $|P_i(j)| > \lambda$, discard (arbitrarily) some packets from $P_i(j)$ to shrink $|P_i(j)|$ to $\lambda$; however, in this case, we say that the algorithm {\em errs}.

    \vgap

    \item For each $j \in [p]$, send $P_i(j)$ to machine $j$ (of course, for $j = i$, ``sending'' $P_i(j)$ requires no communication).
}

For each $i \in [p]$, define
\myeqn{
    P^*_i = \bigcup_{j \in [p]} P_j(i) \nn
}
namely, the set of all packets to be ``relayed'' by machine $i$. These packets are transmitted to machine $i$ in the first round. Machine $i$ sorts the packets by recipient field. For each $j \in [p]$, let $P^*_i(j)$ be the set of packets in $P^*_i$ whose recipient fields are $j$. In the second round, machine $i$, in parallel to others, proceeds as follows:
\myitems{
    \item For each $j \in [p]$, if $|P^*_i(j)| < \lambda$, add dummy packets to $P^*_i(j)$ to increase $|P^*_i(j)|$ to $\lambda$. If $|P^*_i(j)| > \lambda$, discard (arbitrarily) some packets from $P^*_i(j)$ to shrink $|P^*_i(j)|$ to $\lambda$; in this case, we say that the algorithm {\em errs}.

    \vgap

    \item For each $j \in [p]$, send $P^*_i(j)$ to machine $j$.
}
This completes our 2-round algorithm for message routing.

\vgap

In each round, for any distinct $i, j \in [p]$, machine $i$ sends to machine $j$ precisely
\myeqn{
    L' = \lambda \cdot (\delta' + 3) \label{eqn:compile-L'}
}
words. It is rudimentary to verify that $L' \le (1+\delta) L /p$ when $p$ is greater than a constant. Therefore, our 2-round algorithm exhibits a deterministic communication pattern and has a load of $L' (p-1) < (1+\delta) L$. 
If the algorithm does not err, every message $\mb{M}[i,j]$ is successfully delivered to machine $j \in [p]$. In Appendix~\ref{app:compile}, we prove that the algorithm errs with probability at most $1/p^c$ where $c$ can be an arbitrarily large constant chosen before running the algorithm. This completes the proof of Theorem~\ref{thm:compile}.

\bibliographystyle{plainurl}% the mandatory bibstyle
\bibliography{ref}

\appendix

\section*{Appendix}

\section{Basic Concepts of Hypergraphs} \label{app:hypergraph}

Let $\G = (\V, \E)$ be a hypergraph with $\E$ being a multi-set where each element, called an {\em edge}, is a non-empty subset of $\V$ (the set of {\em vertices}). A {\em fractional edge packing} is a function $w: \E \rightarrow [0, 1]$ (mapping each edge $e \in \E$ to a weight $w(e)$ between 0 and 1) such that, for any attribute $X \in \V$, we have $\sum_{e \in \E: X \in e} w(e) \le 1$ (i.e., the weight sum of all the edges containing $X$ is at most 1). The {\em total weight} of $w$ is $\sum_{e \in \E} w(e)$, i.e., the weight sum of all the edges in $\E$. The {\em fractional edge packing number} of $\G$, denoted as $\tau(\G)$, is defined as the maximum total weight of all the fractional edge packings of $\G$.

\vgap

Dual to fractional edge packing is the notion of {\em fractional vertex cover}, which is a function $w: \V \rightarrow [0, 1]$ (mapping each vertex $X \in \V$ to a weight $w(X)$ between 0 and 1) such that, for any edge $e \in \E$, we have $\sum_{X \in \V: X \in e} w(X) \ge 1$ (i.e., the weight sum of all the vertices in $e$ is at least 1). The {\em total weight} of $w$ is $\sum_{X \in \V} w(X)$, i.e., the weight sum of all the vertices in $\V$. The {\em fractional vertex cover number} of $\G$ is defined as the minimum total weight of all the fractional vertex covers of $\G$. It can be shown \cite{su97} that the fractional vertex cover number of $\G$ is always identical to the fractional edge packing number $\tau(\G)$.

\vgap

Given a proper subset $\Z \subset \V$, we define $\G_\Z$ as the hypergraph obtained by ``removing'' the attributes of $\Z$ from $\G$. Specifically:
\myitems{
    \item The vertex set of $\G_\Z$ is $\V \setminus \Z$.
    \item The edges of $\G_\Z$ are decided as follows: for each edge $e \in \G$, we add the edge $e \setminus \Z$ to $\G_\Z$ if $e \setminus \Z \ne \emptyset$.
}
We will refer to $\G_\Z$ as a {\em residual graph} of $\G$. Specially, if $\Z = \emptyset$, then $\G_\Z = \G$.

\vgap

For each proper subset $\Z \subset \V$, the residual graph $\G_\Z$ has its own fractional edge packing number $\tau(\G_\Z)$. The maximum such number of all possible $\Z$ is the {\em edge quasi-packing number} $\psi(\G)$ of $\G$, or formally:
\myeqn{
    \psi(\G) &=&
    \max_{\Z \subset \V} \tau(\G_\Z).
    \label{eqn:hypergraph:psi}
}
Due to the equivalence between fractional edge packing number and fractional vertex cover number, the value $\psi(\G)$ can alternatively be defined as the maximum fractional vertex cover number of all residual graphs $G_\Z$ of $\G$.

\section{A Skew-Free Gathering Algorithm} \label{app:sf-gathering-alg}

In this section, we present a deterministic communication-oblivious algorithm to solve the skew-free gathering problem (parameterized by integer $t$) with load $O(N / p^{1/t})$, matching the lower bound in Theorem~\ref{thm:gathering}.

\vgap

Our algorithm is in essence an adaptation of the ``hyper-cube algorithm'' in \cite{au11}. Define $h = \floor{p^{1/t}}$, and order the points in the $t$-dimensional space $[h]^t$ lexicographically. For each $i \in [h^t]$, assign machine $i$ to the $i$-th point in $[h]^t$ under the lexicographic order. Note that some machines (precisely, $p - h^t$ machines) are not assigned to any point.

\vgap

Divide domain $[p]$ into $h$ disjoint intervals $I_1, I_2, ..., I_h$, ordered in ascending order of their starting points, such that:
\myitems{
    \item each of $I_1, ..., I_{h-1}$ covers $\floor{p/h}$ integers;
    \item $I_h$ covers $p - (h-1) \floor{p/h}$ integers.
}
For each interval $I_c$ with $c \in [h]$, we use $|I_c|$ to denote the number of integers covered by $I_c$. Consider any point $(c_1, c_2, ..., c_t) \in [h]^t$. Remember that the point corresponds to a machine. We associate the machine with a $t$-dimensional box $I_{c_1} \times I_{c_2} \times ... \times I_{c_t}$ and will refer to $I_{c_j}$ the machine's {\em projection} on dimension $j \in [t]$.

\vgap

Given a legal partition of the skew-free gathering problem, each machine --- say the one with id $i \in [p]$ --- sends the $N/p$ elements in its local storage to every machine whose projection on {\em at least} one dimension covers the value $i$ (there are less than $t \cdot h^{t-1}$ such machines). This completes the description of our algorithm.

\vgap

To see the algorithm's correctness, suppose that machines where the special elements are initially placed have ids $z_1, z_2, ..., z_t \in [p]$. For each $j \in [t]$, let $c_j$ be the integer in $[h]$ such that the interval $I_{c_j}$ covers $z_j$. Our algorithm ensures that the machine associated with the $t$-dimensional box $I_{c_1} \times I_{c_2} \times ... \times I_{c_t}$ receives all the special elements from the machines $z_1$, ..., $z_t$. The algorithm's communication obliviousness can be verified from the fact that, for each $i \in [p]$, machine $i$ sends exactly $N/p$ elements to a set of machines that is determined in a way independent of how the input elements are initially distributed.

\vgap

It remains to analyze the algorithm's load. In terms of sending, each machine delivers $N/p$ elements to less than $t \cdot h^{t-1} = O(h^{t-1}) = O(p^{(t-1)/t})$ machines and, thus, transmits $O((N/p) \cdot p^{(t-1)/t}) = O(N/p^{1/t})$ words. In terms of receiving, the machine corresponding to the $t$-dimensional box $I_{c_1} \times I_{c_2} \times ... \times I_{c_t}$ receives at most $(N/p) \cdot \sum_{j=1}^t |I_{c_j}|$ elements. Among all the intervals $I_1, ..., I_h$, the interval $I_h$ is the longest and covers
\myeqn{
    p - (h-1) \lf \fr{p}{h} \rf < p - (h-1) \left( \fr{p}{h} - 1 \right)
    < \fr{p}{h} + h = O(p^{1-1/t}) \nn
}
integers. Each machine, therefore, receives $O((N/p) \cdot p^{1-1/t}) = O(N / p^{1/t})$ words. This proves that our algorithm has a load of $O(N / p^{1/t})$.

\section{Completing the Proof of Thm.~\ref{thm:gathering}} \label{app:proof:thm:gathering}

\noindent {\bf Proof of Inequality (\ref{eqn:gathering:total-X-Y:help2}).} We adopt a counting argument similar to the one used to prove \eqref{eqn:gathering:total-X-Y:help1}. Initially, create an empty set $S_i$ for each $i \in [p]$. Then, process each $Z = (z_1, ..., z_t) \in \YY$ as follows. If $Y(Z, \mb{\Lambda}) = 0$, do nothing. Otherwise, under the communication pattern $\mb{L} = \mb{\Lambda}$, at least one machine $z_k$, for some $k \in [t]$, receives $N/p$ special elements from each machine $z_j$ with $j \in [t] \setminus \set{k}$. This means that all the machines in $Z \setminus \set{z_k}$ are heavy senders for machine $z_k$ under $\mb{\Lambda}$. Thus, $Z \setminus \set{z_k}$ represents a possible way to choose $t-1$ machines from the $H(\mb{\Lambda}, z_k)$ heavy senders for machine $z_k$ under $\mb{\Lambda}$. Accordingly, we add $Z$ to $S_k$. After all the sets $Z \in \YY$ have been processed, the left-hand side of \eqref{eqn:gathering:total-X-Y:help2} is bounded by $\sum_{i \in [p]} |S_i|$. On the other hand, for each $i \in [p]$, the size $|S_i|$ cannot exceed the total number of ways of choosing $t - 1$ machines from the $H(\mb{\Lambda}, i)$ heavy senders for machine $i$ under $\mb{\Lambda}$. Inequality \eqref{eqn:gathering:total-X-Y:help2} now follows.

\extraspacing {\bf Proof of \eqref{eqn:gathering:goal2} When $|\YY| \ge \fr{1}{2} \binom{p}{t}$.} By the definition of the Y-partition, the fact $|\YY| \ge \fr{1}{2} \binom{p}{t}$ yields
\myeqn{
    \sum_{Z \in \YY} \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot
    Y(Z, \mb{\Lambda}) \ge \fr{1}{24} \binom{p}{t}.
    \label{eqn:gathering:Y-help1}
}
Then, we can obtain
\myeqn{
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot \sum_{i \in [p]} H(\mb{\Lambda},i)^{t-1}
    &\ge&
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot \sum_{i \in [p]} \binom{H(\mb{\Lambda},i)}{t-1}
    \nn \\
    \explain{by \eqref{eqn:gathering:total-X-Y:help2}}
    &\ge&
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot \sum_{Z \in \YY} Y(Z, \mb{\Lambda})
    \nn \\
    &=&
    \sum_{\mb{\Lambda} \in \L} \sum_{Z \in \YY} \Pr[\mb{L} = \mb{\Lambda}] \cdot  Y(Z, \mb{\Lambda})
    \nn \\
    \explain{by \eqref{eqn:gathering:Y-help1}}
    &\ge&
    \fr{1}{24} \binom{p}{t}.
    \label{eqn:gathering:Y-help2}
}
This leads to:
\myeqn{
    %\text{LHS of \eqref{eqn:gathering:goal2}}
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \sum_{i \in [p]} H(\mb{\Lambda},i)
    &=&
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot \sum_{i \in [p]} \fr{H(\mb{\Lambda},i)^{t-1}}{H(\mb{\Lambda},i)^{t-2}}
    \nn \\
    \explain{by \eqref{eqn:gathering:bound-H}}
    &\ge&
    \sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot \sum_{i \in [p]} \fr{H(\mb{\Lambda},i)^{t-1}}{p^{\fr{(t-1)\cdot(t-2)}{t}}}
    \nn \\
    \explain{by \eqref{eqn:gathering:Y-help2}}
    &\ge&
    \fr{1}{24} \binom{p}{t} / p^{\fr{(t-1)\cdot(t-2)}{t}}
    \nn \\
    &=&
    \Omega\Big(
        \fr{p^t}{p^{\fr{(t-1)\cdot(t-2)}{t}}}
    \Big)
    =
    \Omega\Big(p^{3-2/t} \Big).
    \nn
}
Therefore, at least one $i \in [p]$ satisfies $\sum_{\mb{\Lambda} \in \L} \Pr[\mb{L} = \mb{\Lambda}] \cdot H(\mb{\Lambda},i) = \Omega(p^{3-2/t} / p ) = \Omega (p^{1-1/t})$, as claimed in \eqref{eqn:gathering:goal2}.

\section{Completing the Proof of Thm.~\ref{thm:join}} \label{app:proof:thm:join}

\noindent {\bf Proof of Lemma~\ref{lmm:share}.} As mentioned, Beame et al.\ \cite{bks17c} has proved Lemma~\ref{lmm:share} for the case where $M = |R^*|$. We will utilize their result as a black box to prove the lemma under $|R^*| < M \leq N$.

\vgap

The number of distinct values taken by the tuples in $R^*$ is at most $|R^*|\cdot |schema(R^*)| \leq \alpha |R^*|$. Hence, we can find in the domain $[\alpha N]$ a set $S$ of $\alpha N - \alpha |R^*|$ values that are not taken by any tuple of $R^*$. From $|R^*| < M \leq N$, we know
\myeqn{
    |S| = \alpha N - \alpha |R^*| > \alpha N - (\alpha -1)N -|R^*| = N - |R^*| \geq M - |R^*|. \nn
}
Identify (arbitrarily) $M - |R^*|$ distinct values from $S$ and, for each such value $x$, create a tuple $\bm{u}$ over $\schema(R^*)$ with $\bm{u}(X) = x$ for each $X \in \schema(R^*)$. Let $R'$ be the relation obtained by inserting these $M - |R^*|$ tuples into $R^*$. Note that $|R'|$ has precisely $M$ tuples.

\vgap

It is easy to verify that $\deg_\Y(R^*) = \deg_\Y(R')$ for every non-empty subset $\Y \subseteq \schema(R')$. With this, the skew-free condition \eqref{eqn:share-skewfree} tells us that, for any non-empty subset $\Y \subseteq \schema(R')$, we must have:
\myeqn{
    \deg_\Y(R') \le \fr{|R'|}{{\prod_{i \in [r]: X_i \in \Y} s_i}}. \nn
}

Recall that, for each $i \in [r]$, we have chosen a perfectly random hash function $h_i: [\alpha N] \rightarrow [s_i]$. Given $\bm{b} = (b_1, b_2, ..., b_r)$ $\in [s_1] \times [s_2] \times ... \times [s_r]$, define
\myeqn{
    \text{\em bin $\bm{b'}$}
    &=&
    \set{\bm{u} \in R' \mid \forall i \in [r], h_i(\bm{u}(X_i)) = b_i}.
    \label{eqn:proof:thm:join:help1}
}
Clearly, bin $\bm{b'}$ contains all the tuples in the bin $\bm{b}$ defined in \eqref{eqn:join-share:bin}. By the result of \cite{bks17c}, we know that every bin $\bm{b'}$ in \eqref{eqn:proof:thm:join:help1} contains $O((\log p^* / \log\log p^*)^r \cdot |R'| / p^*) = O((\log p^* / \log\log p^*)^r \cdot M / p^*)$ tuples, where $p^* = \prod_{i=1}^r s_i$. It thus follows that every bin $\bm{b}$ in \eqref{eqn:join-share:bin} must have a size $O((\log p^* / \log\log p^*)^r \cdot M / p^*)$.

\extraspacing {\bf Proof of Equation~\eqref{eqn:join-help1}.} If $\Y \subseteq \Z$, then $\prod_{X \in \Y} s_X(\Z) = 1 \le p$. Otherwise:
\myeqn{
    \prod_{X \in \Y} s_X(\Z) =
    \prod_{X \in \Y \setminus \Z} s_X(\Z)
    \le
    \prod_{X \in \Y \setminus \Z} p^{w_\Z(X) / \tau_\Z}
    \le
    p.
    \nn
}

\extraspacing {\bf Proof of Equation~\eqref{eqn:join-help2}.} Let $e_R(\Z) = \schema(R)\setminus \Z$. Note that $e_R(\Z)$ is an edge in the residual graph $\G_\Z$. We have
\myeqn{
    \prod_{X \in \schema(R)} s_X(\Z)
    &=&
    \prod_{X \in e_R(\Z)} s_X(\Z) \cdot \prod_{X \in \Z} s_X(\Z) \nn\\
    &=& \prod_{X \in e_R(\Z)} s_X(\Z) \nn \\
    &=& \prod_{X \in e_R(\Z)} \lf p^{w_\Z(X) / \tau(\G_\Z)} \rf \nn \\
    \explain{as $p^{w_\Z(X) / \tau(\G_\Z)} \geq 1$}&\ge& \prod_{X \in e_R(\Z)} \fr{p^{w_\Z(X) / \tau(\G_\Z)}}{2} \nn \\
   \explain{as $|e_R(\Z)|\leq |\schema(R)|\leq \alpha $} &\ge&
   \fr{1}{2^\alpha} \prod_{X \in e_R(\Z)} p^{w_\Z(X) / \tau(\G_\Z)} \nn \\
    &\ge& p^{1/\tau(\G_\Z)} / 2^{\alpha} \nn
}
where the last inequality used the fact that $w_\Z$ is a fractional vertex cover of $G_\Z$ (and, hence, $\sum_{X \in {e_R(\Z)}} w_\Z(X) \ge 1$).

\section{Completing the Proof of Thm.~\ref{thm:compile}} \label{app:compile}

\noindent {\bf Chernoff Bounds.} The following Chernoff bounds (proved in \cite{stl12}) will be useful:

\begin{lemma} \label{lmm::chernoff}
    Let $X_1, X_2, ..., X_f$ be independent Bernoulli random variables that are identically distributed. Define $X = \sum_{i \in [f]} X_i$. For any $\gamma \geq 2$, it holds that
    \myeqn{
        \Pr[X \geq \gamma \cdot \expt[X]] &\leq&  \exp(-\gamma \cdot \expt[X] / 6). \label{eqn:compile:chernoff1}
    }
    For any $0 < \gamma < 1$, it holds that
    \myeqn{
        \Pr[X \geq (1 + \gamma) \expt[X]] &\leq&  \exp(- \gamma^2 \cdot \expt[X] / 3). \label{eqn:compile:chernoff2}
    }
\end{lemma}

Fix an arbitrary real value $\gamma$ satisfying $0 < \gamma < 1$ and any positive real values $p$ and $\beta$. Let us specialize the setup of random variables in the above lemma as follows:
\myitems{
    \item $\Pr[X_i = 1] = 1/p$ for each $i \in [f]$;
    \item $f \le F$ where $F \ge \beta \cdot p \log p$.
}
We claim:
\myeqn{
    \Pr[X \geq (1 + \gamma) F/p] &\leq&  \exp(- \Omega(\gamma^2 \beta \cdot \log p)) \label{eqn:compile:chernoff3}
}
where the hidden constant in the big-$\Omega$ does not depend on $\gamma$, $p$, or $\beta$. Next, we will prove the claim by distinguishing two cases.

\vgap

{\em Case 1: $\expt[X] \ge F / (2p)$.} We have:
\myeqn{
    \Pr[X \geq (1 + \gamma) F/p] &\le& \Pr[X \geq (1 + \gamma) \expt[X]] \nn \\
    \explain{by \eqref{eqn:compile:chernoff2}} &\leq&  \exp(- \gamma^2 \cdot \expt[X] / 3) \nn \\
    &\leq&  \exp(- \gamma^2 \cdot F / (6p)) \nn \\
    &\leq&  \exp(- (\gamma^2 \beta / 6) \cdot \log p) \nn
}
as claimed.

{\em Case 2: $\expt[X] < F / (2p)$.} We can derive:
\myeqn{
    \Pr\left[X \geq (1 + \gamma) F/p\right] &=&
    \Pr\left[X \geq \fr{(1 + \gamma)}{\expt[X] / (F/p)} \cdot \expt[X] \right] \nn \\
    \explain{by \eqref{eqn:compile:chernoff1}} &\leq&  \exp(- (1 + \gamma) \cdot F/(6p)) \nn \\
    &\leq&  \exp(- (\beta / 6) \cdot \log p) \nn
}
as claimed.

\extraspacing {\bf Completing the Analysis of Our Message-Routing Algorithm.} As discussed in Section~\ref{sec:compile}, it remains to prove that our algorithm errs with probability at most $1/p^c$ where $c$ can be an arbitrarily large constant chosen before running the algorithm.

\vgap

For the algorithm to err in the first round, $|P_i(j)|$ must exceed $\lambda$ (see its value in \eqref{eqn:compile-lambda}) for some $i, j \in [p]$. Set $f = |P_i|$ and $F = p + L / \delta'$; we thus have $f \le F$ by \eqref{eqn:compile-P_i-size}. For each $k \in [f]$, introduce a Bernoulli random variable $X_k$ that equals 1 if the $k$-th element of $P_i$ is relayed to machine $j$, and 0 otherwise. Thus, $\Pr[X_k = 1] = 1/p$, and $|P_i(j)| = \sum_{k \in [f]} X_k$.

\vgap

Suppose that $L \ge \beta \cdot p \log p$ where $\beta$ is a constant to be decided later. Note that this means $F > L / \delta' \ge (\beta/\delta') p \log p$.
We can derive:
\myeqn{
    \Pr[|P_i(j)| \ge \lambda]
    &\le&
    \Pr \left[|P_i(j)| \ge \left(1 + \fr{L}{\delta' p} \right) (1+\delta/2) \right] \nn \\
    &=&
    \Pr \left[|P_i(j)| \ge (1+\delta/2) \cdot F/p \right] \nn \\
    \explain{by \eqref{eqn:compile:chernoff3}}
    &\le&
    \exp(- \Omega(\delta^2 \cdot (\beta/\delta') \cdot \log p))  \nn \\
    &=&
    \exp(- \Omega(\delta^3 \cdot \beta \cdot \log p))
    \label{eqn:compile:chernoff4}
}
which can be made less than $1/p^{c_0}$ for an arbitrarily large constant $c_0$ by setting $\beta$ sufficiently large (recall that $\delta$ is a constant).

\vgap

How likely the algorithm errs in the second round is somewhat less obvious. For each $j \in [p]$, denote by $P^+_j$ the set of packets --- produced from $\mb{M}[1,j]$, $\mb{M}[2,j]$, ..., $\mb{M}[p,j]$ combined --- destined for machine $j$ (i.e., those packets have $j$ as the recipient). Because of \eqref{eqn:compile-len-constraint}, $|P^+_j|$, the number of packets in $P^+_j$, is at most $p + L/\delta'$, following a derivation similar to that in \eqref{eqn:compile-P_i-size}. The algorithm errs in the second round only if $|P^*_i(j)| > \lambda$ for some $i, j \in [p]$. Observe that $|P^*_i(j)|$ is precisely the number of packets in $P^+_j$ that have $i$ as their relay machines. Now, set $f = |P^+_j|$ and $F = p + L / \delta'$; we thus have $f \le F$ as just explained. For each $k \in [f]$, introduce a Bernoulli random variable $X_k$ that equals 1 if the $k$-th element of $P^+_j$ is relayed to machine $i$, and 0 otherwise. Thus, $\Pr[X_k = 1] = 1/p$, and $|P^*_i(j)| = \sum_{k \in [f]} X_k$. Following a derivation similar to the one leading to \eqref{eqn:compile:chernoff4}, we obtain
\myeqn{
    \Pr[|P^*_i(j)| \ge \lambda]
    &\le&
    \exp(- \Omega(\delta^3 \cdot \beta \cdot \log p))   \label{eqn:compile:chernoff5}
}

We can now conclude from \eqref{eqn:compile:chernoff4} and \eqref{eqn:compile:chernoff5} that, when $L \ge \beta \cdot p \log p$ for a sufficiently large $\beta$, our algorithm errs with a probability at most $1 / p^c$.

\section{Proof of Theorem \ref{thm:token}} \label{app:thm:token}

Let us start by constructing a set of $p(p-1)$ parameterized inputs. Denote by $\Pi$ the set of $p(p-1)$ ordered pairs $(x, y) \in [p] \times [p]$ satisfying $x \ne y$. For every $x \in [p]$, build a legal partition $\pi_x$ for token-passing by placing $\ceil{p^{1-\eps}}$ tokens on machine $x$ and placing $\ceil{p^{1-\eps}}$ innocuous elements on every other machine. Each pair $(x, y) \in \Pi$ then specifies a distinct parameterized input --- with legal partition $\pi_x$ and parameters $x, y$ --- for which the objective is to move all the tokens from machine $x$ to machine $y$.

\vgap

Let $\A$ be a two-round communication oblivious algorithm that succeeds with probability at least $2/3$ on each of the $p(p-1)$ parameterized inputs defined by $\Pi$. We will prove that $\A$ must entail a load of $\Omega(p^{1-\eps/2})$ with probability at least $1/2$.

\vgap

As $\A$ performs two rounds, its communication pattern can be represented as two $p \times p$ matrices $\mb{L}_1$ and $\mb{L}_2$. Specifically, for each $r = 1$ and $2$, the value $\mb{L}_r[i, j]$ (where $i, j \in [p]$) is the number of words that machine $i$ sends to machine $j$ in round $r$. If $\A$ is randomized, then $\mb{L}_1$ and $\mb{L}_2$ are random variables. For each $r =1$ and 2, we define a $p \times p$ matrix $\mb{L}^*_r$ from $\mb{L}_r$ as follows:
\myitems{
    \item $\mb{L}^*_r[i, j] = \mb{L}_r[i, j]$ for any $i, j \in [p]$ satisfying $i \ne j$.
    \item $\mb{L}^*_r[i, i] = \ceil{p^{1-\eps}}$ for each $i \in [p]$.
}
We remind the reader that $\mb{L}_r[i,i] = 0$ for all $i \in [p]$, by definition of communication pattern. The matrix $\mb{L}^*_r$ thus defined is a random variable decided by $\mb{L}_r$.

\vgap

By communication obliviousness, the distribution of $(\mb{L}_1, \mb{L}_2)$ is the same for all the parameterized inputs specified by the pairs in $\Pi$. Let $\L$ be the set of ordered pairs $(\mb{\Lambda}_1$, $\mb{\Lambda}_2)$ satisfying
\myitems{
    \item each of $\mb{\Lambda}_1$ and $\mb{\Lambda}_2$ is a $p \times p$ matrix; 
    
    
    \item $\Pr[(\mb{L}^*_1$, $\mb{L}^*_2) = (\mb{\Lambda}_1$, $\mb{\Lambda}_2)] > 0$, namely, $\A$ exhibits with a non-zero probability a pattern $(\mb{L}_1$, $\mb{L}_2)$ such that the corresponding $(\mb{L}^*_1$, $\mb{L}^*_2)$ equals $(\mb{\Lambda}_1$, $\mb{\Lambda}_2)$;

    \item the following inequality holds for all $k \in [p]$:
    \myeqn{
        \Big(\sum_{i \in [p]} \mb{\Lambda}_1 [i,k] \Big) \cdot \Big(\sum_{j \in [p]} \mb{\Lambda}_2 [k,j] \Big) \le p^{2-\eps}/24. \label{eqn:thm:token:help-1}
    }
}
The core of our argument is to prove
\myeqn{
    \Pr[(\mb{L}^*_1, \mb{L}^*_2) \notin \L] \ge 1/2.
    \label{eqn:thm:token:help-2}
}
Note that whenever $(\mb{L}^*_1, \mb{L}^*_2) \notin \L$, it follows from the definition in \eqref{eqn:thm:token:help-1} that there is at least one $k \in [p]$ such that
\myeqn{
    \Big(\sum_{i \in [p]} \mb{L}^*_1 [i,k] \Big) \cdot \Big(\sum_{j \in [p]} \mb{L}^*_2 [k,j] \Big) > p^{2-\eps}/24 \nn
}
under which one of the following inequalities must be true:
\myitems{
    \item $\sum_{i \in [p]} \mb{L}^*_1 [i,k] > p^{1-\eps/2} / \sqrt{24}$, or
    \item $\sum_{j \in [p]} \mb{L}^*_2 [k,j] > p^{1-\eps/2} / \sqrt{24}$.
}
In the former case, we have
\myeqn{
    \sum_{i \in [p]} \mb{L}_1 [i,k] &=& \Big(\sum_{i \in [p]} \mb{L}^*_1 [i,k] \Big) - \mb{L}^*_1 [k,k] \nn \\
    &=& \Big(\sum_{i \in [p]} \mb{L}^*_1 [i,k] \Big) - \ceil{p^{1-\eps}}
    = \Omega \Big(p^{1-\eps/2}\Big). \nn
}
This means that the load of $\A$ is $\Omega(p^{1-\eps/2})$ as $\sum_{i \in [p]} \mb{L}_1 [i,k]$ is the number of words received by machine $k$ in the first round. In the latter case, following a similar derivation one can obtain $\sum_{j \in [p]} \mb{L}_2 [k,j] = \Omega(p^{1-\eps/2})$. This also implies that $\A$ has load $\Omega(p^{1-\eps/2})$ because $\sum_{j \in [p]} \mb{L}_2 [k,j]$ is the number of words sent by machine $k$ in the second round. Therefore, with probability at least $1/2$, algorithm $\A$ demands a load of $\Omega(p^{1-\eps/2})$, which completes the proof of Theorem~\ref{thm:token}.

\extraspacing {\bf Proof of Inequality \eqref{eqn:thm:token:help-2}.} Assume for contradiction purposes that \eqref{eqn:thm:token:help-2} does not hold, which means $\Pr[(\mb{L}^*_1, \mb{L}^*_2) \in \L] > 1/2$.

\vgap

For every parameterized input specified by $(x, y) \in \Pi$, we know
\myeqn{
    \Pr[\text{$\A$ succeeds on $(x, y)$} \mid (\mb{L}^*_1, \mb{L}^*_2) \in \L] \ge 1/6.
    \label{eqn:thm:token:help-4}
}
Otherwise, $\Pr[\text{$\A$ succeeds on $(x, y)$}] < 1/6 + \Pr[(\mb{L}^*_1, \mb{L}^*_2) \notin \L] < 2/3$, contradicting the fact that $\A$ succeeds on any input with probability at least $2/3$.

\begin{lemma} \label{lmm:thm:token:help-1}
    If $\A$ succeeds on the parameterized input specified by $(x, y) \in \Pi$ under the condition $(\mb{L}^*_1, \mb{L}^*_2)  = (\mb{\Lambda}_1, \mb{\Lambda}_2)$, then
    \myeqn{
        \sum_{k \in [p]} \min\big\{\mb{\Lambda}_1[x, k], \mb{\Lambda}_2[k, y] \big\} \ge p^{1-\eps}.
        \label{eqn:thm:token:help-3}
    }
\end{lemma}

\begin{proof}
    Let us rewrite the left hand side of \eqref{eqn:thm:token:help-3} as
    \myeqn{
        \hspace{-5mm}
        && \sum_{k \in [p]} \min\big\{\mb{\Lambda}_1[x, k], \mb{\Lambda}_2[k, y] \big\} \nn \\
        \hspace{-5mm}& = &
        \Big( \sum_{k \in [p] \setminus \set{x,y}} \min\big\{\mb{\Lambda}_1[x, k], \mb{\Lambda}_2[k, y] \big\} \Big)
        \nn \\
        \hspace{-5mm}&& + \min\big\{\mb{\Lambda}_1[x, y], \ceil{p^{1-\eps}} \big\}
        + \min\big\{\ceil{p^{1-\eps}}, \mb{\Lambda}_2[x, y] \big\}.
        \label{eqn:thm:token:help-8}
    }

    If a machine $k \in [p] \setminus \set{x, y}$ receives a token from machine $x$ in the first round and sends the token to machine $y$ in the second round, we say that the token is {\em relayed} by machine $k$. Under the condition $(\mb{L}^*_1, \mb{L}^*_2)  = (\mb{\Lambda}_1, \mb{\Lambda}_2)$, machine $k$ can relay no more than $\min\{\mb{\Lambda}_1[x, k], \mb{\Lambda}_2[k, y] \}$ tokens. On the other hand, if a token is not relayed by any machines, it needs to be sent from machine $x$ to machine $y$ directly (in either the first or the second round). The number of tokens that can be sent this way is at most
    \myeqn{
        \min\big\{\mb{\Lambda}_1[x, y], \ceil{p^{1-\eps}} \big\}
        + \min\big\{\ceil{p^{1-\eps}}, \mb{\Lambda}_2[x, y] \big\}. \nn
    }

    Thus, \eqref{eqn:thm:token:help-8} gives an upper bound on the total number of tokens that $\A$ can move to machine $y$ at the end of the second round. The upper bound must be at least $\ceil{p^{1-\eps}}$ for $\A$ to succeed on $(x, y)$.
\end{proof}

By putting together \eqref{eqn:thm:token:help-4} and \eqref{eqn:thm:token:help-3}, we obtain the following for every parameterized input specified by $(x, y) \in \Pi$:
\myeqn{
    \expt\Big[\sum_{k \in [p]} \min\big\{\mb{L}^*_1[x, k], \mb{L}^*_2[k, y] \big\} \bigmid (\mb{L}^*_1, \mb{L}^*_2) \in \L \Big]
    \ge
    \fr{p^{1-\eps}}{6}
    \label{eqn:thm:token:help-5}
}

For any $(\mb{\Lambda}_1, \mb{\Lambda}_2) \in \L$ and any $k \in [p]$, we have: 
\myeqn{
    && \Big( \sum_{x \in [p]} \mb{\Lambda}_1[x,k] \Big)
    \cdot \Big( \sum_{y \in [p]} \mb{\Lambda}_2[k,y] \Big) \nn \\
    &=&
    \sum_{x \in [p]} \sum_{y \in [p]}
     \mb{\Lambda}_1[x,k] \cdot \mb{\Lambda}_2[k,y]  \nn \\
    &\ge& 
    \sum_{x \in [p]} \sum_{y \in [p]}
    \min\big\{\mb{\Lambda}_1[x,k], \mb{\Lambda}_2[k,y]\big\}  \nn \\ 
    &\ge& 
    \sum_{(x,y) \in \Pi} 
    \min\big\{\mb{\Lambda}_1[x,k], \mb{\Lambda}_2[k,y]\big\}  \label{eqn:thm:token:help-6}
}
where the second inequality used the fact that $ab \ge \min\{a, b\}$ holds for any non-negative integers $a$ and $b$.

\vgap

We can now derive:
\myeqn{
    && \sum_{k \in [p]} \expt \Big[ \Big(\sum_{x \in [p]} \mb{L}^*_1[i, k] \Big) \cdot \Big( \sum_{y \in [p]} \mb{L}^*_2[k, j] \Big)
    \bigmid
    (\mb{L}^*_1, \mb{L}^*_2) \in \L
    \Big] \nn \\
    &\ge&
    \expt \Big[\sum_{k \in [p]} \sum_{(x,y) \in \Pi} \min\big\{\mb{L}^*_1[x,k], \mb{L}^*_2[k,y]\big\}
    \bigmid
    (\mb{L}^*_1, \mb{L}^*_2) \in \L
    \Big] \nn \\
    && \explain{by linearity of expectation and \eqref{eqn:thm:token:help-6}} \nn \\
    &=&
    \expt \Big[\sum_{(x,y) \in \Pi} \sum_{k \in [p]} \min\big\{\mb{L}^*_1[x,k], \mb{L}^*_2[k,y]\big\}
    \bigmid
    (\mb{L}^*_1, \mb{L}^*_2) \in \L
    \Big] \nn \\
    &=&
    \sum_{(x,y) \in \Pi}
    \expt \Big[\sum_{k \in [p]} \min\big\{\mb{L}^*_1[x,k], \mb{L}^*_2[k,y]\big\}
    \bigmid
    (\mb{L}^*_1, \mb{L}^*_2) \in \L
    \Big] \nn \\
    && \explain{linearity of expectation} \nn \\ 
    &\ge& 
    \fr{p^{1-\eps} \cdot p (p-1)}{6}. \hspace{5mm} \explain{by \eqref{eqn:thm:token:help-5}} \nn 
}

Therefore, there is at least one $k^* \in [p]$ such that 
\myeqn{
    && \expt \Big[ \Big(\sum_{x \in [p]} \mb{L}^*_1[i, k^*] \Big) \cdot \Big( \sum_{y \in [p]} \mb{L}^*_2[k^*, j] \Big)
    \bigmid
    (\mb{L}^*_1, \mb{L}^*_2) \in \L
    \Big] \nn \\ 
    &\ge& 
    \fr{p^{1-\eps}(p-1)}{6} \ge p^{2-\eps}/12
    \label{eqn:thm:token:help-7}
}
where the last inequality used $p \ge 2$. 

\vgap 

However, by the definition of $\L$ (in particular, the inequality \eqref{eqn:thm:token:help-1}), when $ (\mb{L}^*_1, \mb{L}^*_2) \in \L $, the product $\sum_{x \in [p]} \mb{L}^*_1[i, k]  \cdot \sum_{y \in [p]} \mb{L}^*_2[k, j]$ can be at most $p^{2-\eps}/24$ for all $k \in [p]$. This contradicts the existence of $k^*$. Therefore, our assumption
$\Pr[(\mb{L}^*_1, \mb{L}^*_2) \in \L] > 1/2$ must be a false one.

\balance

%\end{sloppy}
\end{document}

